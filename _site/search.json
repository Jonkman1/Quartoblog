[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NSC-R Tidy Tuesday workshops",
    "section": "",
    "text": "EDA on salary survey in different countries\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nFranziska Yasrebi-de Kom\n\n\n\n\n\n\n  \n\n\n\n\nStudent mobility in Europe\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nWim Bernasco\n\n\n\n\n\n\n  \n\n\n\n\nDevelopment of crime in population\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2022\n\n\nWim Bernasco\n\n\n\n\n\n\n  \n\n\n\n\nRevenue and expenditure in sport\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2022\n\n\nAlex Trinidad\n\n\n\n\n\n\n  \n\n\n\n\nLong to wide (and vv) transformations\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\nSam Langton\n\n\n\n\n\n\n  \n\n\n\n\nResearching Spanish soccer data\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2022\n\n\nWim Bernasco\n\n\n\n\n\n\n  \n\n\n\n\nSankey diagrams and how to create\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2022\n\n\nTim Verlaan\n\n\n\n\n\n\n  \n\n\n\n\nWrangling dates-time data\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2022\n\n\nAlex Trinidad\n\n\n\n\n\n\n  \n\n\n\n\nResearch of Board Games\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nAsier Moreva\n\n\n\n\n\n\n  \n\n\n\n\nExploration and visualization of Starbuck coffee data\n\n\n\n\n\n\n\nexploration\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2022\n\n\nSam Langton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThe NSC-R Tidy Tuesday workshop sessions are inspired by the Tidy Tuesday initiative, which was aimed at providing a safe and supportive forum for individuals to practice their data processing and visualization skills in R while working with real-world data.Here ten workshops are presented in ten posts\nIn the first post Sam Langton presents the exploration and visualization of Starbuck coffee-use data.\nThe second post is about board games and here Asier Moneva explores research questions like ‘What are the top 3 best-selling board games by game type?’ and ‘How do board game sales relate to online user ratings’. He shows how this can be explored and visualized.\nAlex Trinidad shows us in post nr. 3 how R and RStudio can be used to wrangle dates-time data.\nPost nr. 4 introduces us to Sankey diagrams, and how they can be created in R. Tim Verlaan is our mentor in this workshop.\nWim Bernaso introduces us in Post 5 to open footbal data which can be explored at various levels (player, team, match, competition) and from various perspectives.\nPost 6 (Sam Langton) demonstrates long to wide (and wide to long) transformations using functions available in the tidyrpackage. In this chapter data from the London Fire Brigade are used.\nPost 7 explores how revenue and expenditure are distributed in sports. In this chapter Alex Trinidad let us look at the differences in sport revenues and expenditures between men and women.\nIn post nr. 8 Wim Bernasco tries to make sense of long-term temporal trends in crimes, and to make useful statements about how things changed when the COVID pandemic arrived around February 2020. In this chapter eight steps in the analysis are worked out.\nAlso in post nr. 9 Wim Bernasco is the workshopleader. In this workshop, the focus is on exploring, analyzing, and visualizing student mobilisation streams between countries.\nPost nr. 10 is the last workshop presented here and this one is given by Franziska Yasrebi-de Kom, and is a descriptive analysis on a salary survey in different countries and some background variables of the participants."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html",
    "href": "posts/01_TT/01_TT_202201.html",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "",
    "text": "On 11 January 2022 and 25 January 2022 Sam Langton gave NSCR Tidy Tuesday presentations on the exploration and visualization of Starbuck coffee use data.In this document both presentations are combined (Langton 2022).\n\n\nHere you can find it on the NSCR- website. Here is the First presentation. Here is the Second presentation"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#load-libraries.",
    "href": "posts/01_TT/01_TT_202201.html#load-libraries.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Load libraries.",
    "text": "Load libraries.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#load-data",
    "href": "posts/01_TT/01_TT_202201.html#load-data",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Load data",
    "text": "Load data\nThe data are loaded directly from the TidyTuesday github page.\n\nstar_df <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv')"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#initial-explore.",
    "href": "posts/01_TT/01_TT_202201.html#initial-explore.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Initial explore.",
    "text": "Initial explore.\nHere are some explorative commands you can use: - star-df opens the dataset and shows you the variables.\n- names(star_df)gives you the names of the 15 variables.\n- glimpse(star_df)shows you the number of rows (often particpants) and columns (often variables) and also informs you about the kind of variables.\n- dim(star_df) informs you about number of rows and columns.\n- sum(is.na(star_df)) show you the number of missing variables.\n\nstar_df\n\n# A tibble: 1,147 x 15\n   product_n~1 size   milk  whip serv_~2 calor~3 total~4 satur~5 trans~6 chole~7\n   <chr>       <chr> <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>     <dbl>\n 1 brewed cof~ short     0     0     236       3     0.1       0 0             0\n 2 brewed cof~ tall      0     0     354       4     0.1       0 0             0\n 3 brewed cof~ gran~     0     0     473       5     0.1       0 0             0\n 4 brewed cof~ venti     0     0     591       5     0.1       0 0             0\n 5 brewed cof~ short     0     0     236       3     0.1       0 0             0\n 6 brewed cof~ tall      0     0     354       4     0.1       0 0             0\n 7 brewed cof~ gran~     0     0     473       5     0.1       0 0             0\n 8 brewed cof~ venti     0     0     591       5     0.1       0 0             0\n 9 brewed cof~ short     0     0     236       3     0.1       0 0             0\n10 brewed cof~ tall      0     0     354       4     0.1       0 0             0\n# ... with 1,137 more rows, 5 more variables: sodium_mg <dbl>,\n#   total_carbs_g <dbl>, fiber_g <chr>, sugar_g <dbl>, caffeine_mg <dbl>, and\n#   abbreviated variable names 1: product_name, 2: serv_size_m_l, 3: calories,\n#   4: total_fat_g, 5: saturated_fat_g, 6: trans_fat_g, 7: cholesterol_mg\n\nnames(star_df)\n\n [1] \"product_name\"    \"size\"            \"milk\"            \"whip\"           \n [5] \"serv_size_m_l\"   \"calories\"        \"total_fat_g\"     \"saturated_fat_g\"\n [9] \"trans_fat_g\"     \"cholesterol_mg\"  \"sodium_mg\"       \"total_carbs_g\"  \n[13] \"fiber_g\"         \"sugar_g\"         \"caffeine_mg\"    \n\nglimpse(star_df)\n\nRows: 1,147\nColumns: 15\n$ product_name    <chr> \"brewed coffee - dark roast\", \"brewed coffee - dark ro~\n$ size            <chr> \"short\", \"tall\", \"grande\", \"venti\", \"short\", \"tall\", \"~\n$ milk            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, ~\n$ whip            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ serv_size_m_l   <dbl> 236, 354, 473, 591, 236, 354, 473, 591, 236, 354, 473,~\n$ calories        <dbl> 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 35, 50~\n$ total_fat_g     <dbl> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,~\n$ saturated_fat_g <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,~\n$ trans_fat_g     <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\",~\n$ cholesterol_mg  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10,~\n$ sodium_mg       <dbl> 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, ~\n$ total_carbs_g   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, ~\n$ fiber_g         <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\",~\n$ sugar_g         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, ~\n$ caffeine_mg     <dbl> 130, 193, 260, 340, 15, 20, 25, 30, 155, 235, 310, 410~\n\ndim(star_df)\n\n[1] 1147   15\n\nsum(is.na(star_df))\n\n[1] 0"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#subset-brewed-coffee.",
    "href": "posts/01_TT/01_TT_202201.html#subset-brewed-coffee.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Subset brewed coffee.",
    "text": "Subset brewed coffee.\nWhen you want to look at a specific subset with the product_name brewed coffee for example you can define this as:.\n\nbrew_df <- star_df |>\n  filter(str_detect(product_name, \"brewed coffee\"))"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#quick-clean-for-the-plot.",
    "href": "posts/01_TT/01_TT_202201.html#quick-clean-for-the-plot.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Quick clean for the plot.",
    "text": "Quick clean for the plot.\n\nMake another subsample and call it big_ones_df. -Use only the variables product_name, size, milk, whip and calories-trans_fat_g).\n\nUse only when sizeis grande.\nchange whip into a character variable.\nRecode categories of milkfrom 0, 1, 2, 3, 4 5 into no milk, non-fat, 2% fat, soy, coconut, whole.\n\n\nbig_ones_df <- star_df |>\n  select(product_name, size, milk, whip, calories:trans_fat_g) |> \n  filter(size == \"grande\") |>\n  mutate(whip_char = as.character(whip),\n         milk_labs = recode(milk,\n                            `0` = \"no milk\",\n                            `1` = \"non-fat\",\n                            `2` = \"2% fat\",\n                            `3` = \"soy\",\n                            `4` = \"coconut\",\n                            `5` = \"whole\")) # These categories are on the tt git page."
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#what-is-the-relationship-between-calories-and-fat",
    "href": "posts/01_TT/01_TT_202201.html#what-is-the-relationship-between-calories-and-fat",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "What is the relationship between calories and fat?",
    "text": "What is the relationship between calories and fat?\nCan you plot between calories(y) and total_fat_g (x)? Show the diffence on whip_char'and show it for the sixmilk-labs` you recoded.\n\nmy_plot_gg <- ggplot(data = big_ones_df) +\n  geom_point(mapping = aes(x = total_fat_g, y = calories, fill = whip_char),\n             size = 2, alpha = 0.8, pch = 21, colour = \"black\") +\n  facet_wrap(~milk_labs) +\n  labs(title = \"Starbucks: fat, calories and milk types\",\n       caption = \"Data notes: grande drink size | Data source: tidytuesday | NSC-R workshop 11 Jan 2022\",\n       fill = NULL, x = \"total fat (grams)\") +\n  scale_fill_manual(values = c(\"#036635\", \"#b5651d\"),\n                      labels = c(\"Without whipped cream\", \"With whipped cream\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        axis.text = element_text(size = 6),\n        axis.title = element_text(size = 8),\n        plot.title = element_text(hjust = 0.5),\n        plot.caption = element_text(size = 4))\nmy_plot_gg\n\n\n\n\nStarbucks coffeetypes"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#save.",
    "href": "posts/01_TT/01_TT_202201.html#save.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Save.",
    "text": "Save.\nSave the plot on your computer as a png-file. Here is place in a projectmap and a submap images which I made on my computer. Change the path if necessary for your computer.\n\nggsave(my_plot_gg, file = \"images/starbucks_plot.png\",\n       height = 12, width = 12, unit = \"cm\")"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#counting-frequencies",
    "href": "posts/01_TT/01_TT_202201.html#counting-frequencies",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Counting frequencies",
    "text": "Counting frequencies\nFor counting frequencies you always can use different methods. We show some of them.\nThis is the R base-way for counting the variable whipfor example.\n\ntable(star_df$whip)\n\n\n  0   1 \n864 283 \n\n\nThis is the grouping-way.\n\nstar_df |>\n  group_by(whip) |> \n  tally() \n\n# A tibble: 2 x 2\n   whip     n\n  <dbl> <int>\n1     0   864\n2     1   283\n\n\nThis is a better way (thanks Wim!).\n\nfreq_df <- count(star_df, whip)\nfreq_df\n\n# A tibble: 2 x 2\n   whip     n\n  <dbl> <int>\n1     0   864\n2     1   283"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#basic-cleaning",
    "href": "posts/01_TT/01_TT_202201.html#basic-cleaning",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Basic cleaning",
    "text": "Basic cleaning\nHere we do some basic cleaning. We select six variables and look only at variables of which the size is grande. We call this dataset star_clean.\n\nstar_clean_df <- star_df |>\n  select(product_name, size, milk, whip, calories, sugar_g) |>\n  filter(size == \"grande\")"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#basic-plot",
    "href": "posts/01_TT/01_TT_202201.html#basic-plot",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Basic plot",
    "text": "Basic plot\nLet us plot it on different ways.The first three are similar.\n\nggplot(data = star_clean_df, mapping = aes(x = sugar_g, y = calories)) +\n  geom_point()\n\nggplot(data = star_clean_df) +\n  geom_point(mapping = aes(x = sugar_g, y = calories))\n\nggplot() +\n  geom_point(data = star_clean_df, mapping = aes(x = sugar_g, y = calories))\n\n\n\n\n\n\nFirst way\n\n\n\n\n\n\n\nSecond way\n\n\n\n\n\n\n\nThird way\n\n\n\n\n\n\nRelation between sugar and calories\n\n\n\n\nThen we plot the coffee of which the whipis 1.\n\nstar_clean_df |>\n  filter(whip == 1) |>\n  ggplot() +\n  geom_point(mapping = aes(x = sugar_g, y = calories))\n\n\n\n\nRelation between sugar and calories with coffee whip-1"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#change-milk-class.",
    "href": "posts/01_TT/01_TT_202201.html#change-milk-class.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Change milk class.",
    "text": "Change milk class.\nFor the next plot we first change the class of the variable milk into a character variable and save it as a different dataset.\n\nstar_clean_df <- star_clean_df |> \n  mutate(milk_char = as.character(milk))"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#extend.",
    "href": "posts/01_TT/01_TT_202201.html#extend.",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Extend.",
    "text": "Extend.\nWe plot this new dataset on three different ways.\n\nggplot(data = star_clean_df) +\n  geom_point (mapping = aes(x = sugar_g, y = calories)) +\n  geom_smooth(mapping = aes(x = sugar_g, y = calories), method = \"lm\") +\n  facet_wrap(~milk_char)\n\nggplot(data = star_clean_df, mapping = aes(x = sugar_g, y = calories)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(vars(milk_char, whip))\n\n\nggplot(data = star_clean_df) +\n  geom_point(mapping = aes(x = sugar_g, y = calories, shape = milk_char, colour = milk_char))\n\n\n\n\n\n\nFirst way\n\n\n\n\n\n\n\nSecond way\n\n\n\n\n\n\n\nThird way\n\n\n\n\n\n\nRelation between sugar and calories with coffee whip-1"
  },
  {
    "objectID": "posts/01_TT/01_TT_202201.html#reference",
    "href": "posts/01_TT/01_TT_202201.html#reference",
    "title": "Exploration and visualization of Starbuck coffee data",
    "section": "Reference",
    "text": "Reference"
  },
  {
    "objectID": "posts/02_TT/02_TT_202202.html",
    "href": "posts/02_TT/02_TT_202202.html",
    "title": "Research of Board Games",
    "section": "",
    "text": "The dataset for this Tidy Tuesday is about board games! Asier Moneva explores the following research questions: “What are the top 3 best-selling board games by game type?” and “How do board game sales relate to online user ratings?”[@moneva_nsc-r_2022]"
  },
  {
    "objectID": "posts/02_TT/02_TT_202202.html#load-packages",
    "href": "posts/02_TT/02_TT_202202.html#load-packages",
    "title": "Research of Board Games",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nToday’s data are relational, which means there are multiple datasets (rating, details) that can be linked through some identifier or id variable or ‘key’."
  },
  {
    "objectID": "posts/02_TT/02_TT_202202.html#import-data",
    "href": "posts/02_TT/02_TT_202202.html#import-data",
    "title": "Research of Board Games",
    "section": "Import data",
    "text": "Import data\n\nratings <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv\")\n\n\ndetails <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv\")"
  },
  {
    "objectID": "posts/02_TT/02_TT_202202.html#explore-data",
    "href": "posts/02_TT/02_TT_202202.html#explore-data",
    "title": "Research of Board Games",
    "section": "Explore data",
    "text": "Explore data\nSo let’s have a quick look at the data.\n\nglimpse(ratings)\n\nRows: 21,831\nColumns: 10\n$ num           <dbl> 105, 189, 428, 72, 103, 191, 100, 3, 15, 35, 30, 182, 13~\n$ id            <dbl> 30549, 822, 13, 68448, 36218, 9209, 178900, 167791, 1733~\n$ name          <chr> \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders\", \"Domini~\n$ year          <dbl> 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016, 2015, 20~\n$ rank          <dbl> 106, 190, 429, 73, 104, 192, 101, 4, 16, 36, 31, 183, 14~\n$ average       <dbl> 7.59, 7.42, 7.14, 7.74, 7.61, 7.41, 7.60, 8.42, 8.11, 7.~\n$ bayes_average <dbl> 7.487, 7.309, 6.970, 7.634, 7.499, 7.305, 7.508, 8.274, ~\n$ users_rated   <dbl> 108975, 108738, 108024, 89982, 81561, 76171, 74419, 7421~\n$ url           <chr> \"/boardgame/30549/pandemic\", \"/boardgame/822/carcassonne~\n$ thumbnail     <chr> \"https://cf.geekdo-images.com/S3ybV1LAp-8SnHIXLLjVqA__mi~\n\nglimpse(details)\n\nRows: 21,631\nColumns: 23\n$ num                     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ~\n$ id                      <dbl> 30549, 822, 13, 68448, 36218, 9209, 178900, 16~\n$ primary                 <chr> \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders~\n$ description             <chr> \"In Pandemic, several virulent diseases have b~\n$ yearpublished           <dbl> 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016~\n$ minplayers              <dbl> 2, 2, 3, 2, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2, 2, 2~\n$ maxplayers              <dbl> 4, 5, 4, 7, 4, 5, 8, 5, 2, 5, 5, 4, 5, 5, 5, 4~\n$ playingtime             <dbl> 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150~\n$ minplaytime             <dbl> 45, 30, 60, 30, 30, 30, 15, 120, 30, 30, 90, 3~\n$ maxplaytime             <dbl> 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150~\n$ minage                  <dbl> 8, 7, 10, 10, 13, 8, 14, 12, 10, 12, 12, 10, 1~\n$ boardgamecategory       <chr> \"['Medical']\", \"['City Building', 'Medieval', ~\n$ boardgamemechanic       <chr> \"['Action Points', 'Cooperative Game', 'Hand M~\n$ boardgamefamily         <chr> \"['Components: Map (Global Scale)', 'Component~\n$ boardgameexpansion      <chr> \"['Pandemic: Gen Con 2016 Promos – Z-Force Tea~\n$ boardgameimplementation <chr> \"['Pandemic Legacy: Season 0', 'Pandemic Legac~\n$ boardgamedesigner       <chr> \"['Matt Leacock']\", \"['Klaus-Jürgen Wrede']\", ~\n$ boardgameartist         <chr> \"['Josh Cappel', 'Christian Hanisch', 'Régis M~\n$ boardgamepublisher      <chr> \"['Z-Man Games', 'Albi', 'Asmodee', 'Asmodee I~\n$ owned                   <dbl> 168364, 161299, 167733, 120466, 106956, 105748~\n$ trading                 <dbl> 2508, 1716, 2018, 1567, 2009, 930, 1110, 538, ~\n$ wanting                 <dbl> 625, 582, 485, 1010, 655, 692, 340, 2011, 924,~\n$ wishing                 <dbl> 9344, 7383, 5890, 12105, 8621, 6620, 5764, 192~\n\n\nYou see that the number of observations do not match!\nExplore whether both datasets have identical id\n\nidentical(details$id, ratings$id)\n\n[1] FALSE\n\n\nCount how many id are missing.\n\ntable(ratings$id %in% details$id)\n\n\nFALSE  TRUE \n  200 21631 \n\n\nFind which id are present in both datasets. Results not show here because of too long output.\n\nintersect(details$id, ratings$id)\n\nFind which id are absent in the dataset with the least observations\n\nsetdiff(ratings$id, details$id)\n\n  [1] 298130 286790   2415  12369  25999 304051 262554  67499  16268 236881\n [11] 102750  15391  10889   4875 165190 225303  18536 192323 143663  18704\n [21]  33125 297139   7578  32922 143459 237088 311004 195158  12624 201917\n [31]  34403  18061   5043  15049  23733 225828   6913 176633 198541  64608\n [41]   3610  29268 244268 177566  38836 275284  10544 284617 191292  96626\n [51]   8741   7752 136856  12900  30335 189615 183344  36238  39663 314125\n [61] 325678 246310 109827  19475  31412  69233   5649  98978   3814 140973\n [71] 265039  39278 188968 144378 303676  20533  13094   9860 310880 304666\n [81] 279649  40804   8249 245060 246895  10246  14698 172088   6124  20647\n [91] 264654 299690 147457  23616 258761 311990 132616 154841   8356 216360\n[101]  12169 226237 118561  49050 282410   2493   9398   9578   9512  24079\n[111]   3398 147616   7368 170390  36993  23999 316090  11674   9454  34129\n[121] 252901  10105 287304 164428  41611 232874   1550 252373   3909 145206\n[131]  36366 308870  24857 210350   6839 125585   9113 145400 239523  16830\n[141]  19304  40886  21437 328866 244528 252374  17996  22354 283849  14128\n[151] 305270 237834 137366  38524   3338   1221 319593   3830 102150  10154\n[161] 208800 214486  32324  86443 162888 221208 241611  17433   9736  18687\n[171] 104770   8426 247135 233284  11421  13918  31585   2103 210292 276641\n[181] 289055  15427  24994  47475  66276 205156   3047   8700 306072  14377\n[191]   1191 251026  19044 282389 296644 284839 144874 179124 150429  35499\n\n\nMerge data\n\ndf <- inner_join(ratings, details, by = \"id\")\n\nExplore data\n\nglimpse(df)\n\nRows: 21,631\nColumns: 32\n$ num.x                   <dbl> 105, 189, 428, 72, 103, 191, 100, 3, 15, 35, 3~\n$ id                      <dbl> 30549, 822, 13, 68448, 36218, 9209, 178900, 16~\n$ name                    <chr> \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders~\n$ year                    <dbl> 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016~\n$ rank                    <dbl> 106, 190, 429, 73, 104, 192, 101, 4, 16, 36, 3~\n$ average                 <dbl> 7.59, 7.42, 7.14, 7.74, 7.61, 7.41, 7.60, 8.42~\n$ bayes_average           <dbl> 7.487, 7.309, 6.970, 7.634, 7.499, 7.305, 7.50~\n$ users_rated             <dbl> 108975, 108738, 108024, 89982, 81561, 76171, 7~\n$ url                     <chr> \"/boardgame/30549/pandemic\", \"/boardgame/822/c~\n$ thumbnail               <chr> \"https://cf.geekdo-images.com/S3ybV1LAp-8SnHIX~\n$ num.y                   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ~\n$ primary                 <chr> \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders~\n$ description             <chr> \"In Pandemic, several virulent diseases have b~\n$ yearpublished           <dbl> 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016~\n$ minplayers              <dbl> 2, 2, 3, 2, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2, 2, 2~\n$ maxplayers              <dbl> 4, 5, 4, 7, 4, 5, 8, 5, 2, 5, 5, 4, 5, 5, 5, 4~\n$ playingtime             <dbl> 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150~\n$ minplaytime             <dbl> 45, 30, 60, 30, 30, 30, 15, 120, 30, 30, 90, 3~\n$ maxplaytime             <dbl> 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150~\n$ minage                  <dbl> 8, 7, 10, 10, 13, 8, 14, 12, 10, 12, 12, 10, 1~\n$ boardgamecategory       <chr> \"['Medical']\", \"['City Building', 'Medieval', ~\n$ boardgamemechanic       <chr> \"['Action Points', 'Cooperative Game', 'Hand M~\n$ boardgamefamily         <chr> \"['Components: Map (Global Scale)', 'Component~\n$ boardgameexpansion      <chr> \"['Pandemic: Gen Con 2016 Promos – Z-Force Tea~\n$ boardgameimplementation <chr> \"['Pandemic Legacy: Season 0', 'Pandemic Legac~\n$ boardgamedesigner       <chr> \"['Matt Leacock']\", \"['Klaus-Jürgen Wrede']\", ~\n$ boardgameartist         <chr> \"['Josh Cappel', 'Christian Hanisch', 'Régis M~\n$ boardgamepublisher      <chr> \"['Z-Man Games', 'Albi', 'Asmodee', 'Asmodee I~\n$ owned                   <dbl> 168364, 161299, 167733, 120466, 106956, 105748~\n$ trading                 <dbl> 2508, 1716, 2018, 1567, 2009, 930, 1110, 538, ~\n$ wanting                 <dbl> 625, 582, 485, 1010, 655, 692, 340, 2011, 924,~\n$ wishing                 <dbl> 9344, 7383, 5890, 12105, 8621, 6620, 5764, 192~\n\n\nLooks like there are some duplicated variables!\nNot really\n\nidentical(df$year, df$yearpublished)\n\n[1] FALSE\n\n\nWhat is the difference?\n\nwhich(df$year != df$yearpublished)\n\n [1]   209   297   881  1030  2239  3623  4089  4723 15507 16494 21623\n\n\nAh, I see!\n\ndf$year[21623]\n\n[1] 3000\n\ndf$yearpublished[21623]\n\n[1] -3000\n\n\nNope. Very different. Can these be row numbers (e.g. spreadsheet)?\n\nidentical(df$num.x, df$num.y)\n\n[1] FALSE\n\n\nAlmost, but nope.\n\nidentical(df$name, df$primary)\n\n[1] FALSE\n\nwhich(df$name != df$primary)\n\n[1]  7222 10903 12599 13233 19925\n\n\nLooks like there is some sort of syntax issue in primary\n\ndf$name[19925]\n\n[1] \"Admiral Ackbar \\\"It's a TRAP!\\\" GAME\"\n\ndf$primary[19925]\n\n[1] \"Admiral Ackbar \\\"It\\\\'s a TRAP!\\\" GAME\"\n\n\nBingo!\n\nidentical(df$playingtime, df$maxplaytime)\n\n[1] TRUE\n\n\nLet’s drop the duplicated variable\n\ndf <- df %>% \n  select(\n    - playingtime,\n    - primary\n  )\n\nLooks like we are good now! Imagine the following:\n\nWe want to buy a board game for a friend, but we don’t know which one to get.\nWe know our friend likes to play alone with his girlfriend sometimes, but some Other times he likes to invite friends over to play.\nWhat are the top 3 best-selling board games by game type?\nHow do board game sales relate to online user ratings?\nWhat are the top 3 best-selling board games by game type?\n\n\ndf <- df |> \n  # Create new variable to define type of game based on number of players\n  mutate(minplayers_fct = case_when(\n    minplayers < 2 ~ \"single player\",\n    minplayers == 2 ~ \"two players\",\n    minplayers > 2 ~ \"multiplayer\"\n  ))\n\n\ndf |> \n  # Select only relevant variables\n  select(\n    name,\n    minplayers_fct,\n    owned\n  ) |> \n  group_by(minplayers_fct) |> \n  slice_max(owned, n = 3) |> \n  knitr::kable()\n\n\n\n\nname\nminplayers_fct\nowned\n\n\n\n\nCatan\nmultiplayer\n167733\n\n\nMunchkin\nmultiplayer\n78849\n\n\nDixit\nmultiplayer\n76535\n\n\nTerraforming Mars\nsingle player\n101872\n\n\nScythe\nsingle player\n86371\n\n\nWingspan\nsingle player\n83920\n\n\nPandemic\ntwo players\n168364\n\n\nCarcassonne\ntwo players\n161299\n\n\n7 Wonders\ntwo players\n120466\n\n\n\n\n\nWe have identified the best-sellers for each category, but best-sellers might be a product of marketing rather than an indicator of good quality. It might be the case that players buy the product because of good marketing but then find the game boring, too complicated, etc.\nHow do board game sales relate to online user ratings?"
  },
  {
    "objectID": "posts/02_TT/02_TT_202202.html#plot-data",
    "href": "posts/02_TT/02_TT_202202.html#plot-data",
    "title": "Research of Board Games",
    "section": "Plot data",
    "text": "Plot data\n\ndf %>% \n  ggplot(mapping = aes(\n    x = average,\n    y = owned,\n    # Examine if there are clear patterns by number of players\n    color = minplayers_fct\n  )) +\n  geom_point(alpha = .5) +\n  # There seems to be three outliers that are best-sellers and also have high \n  # ratings\n  ggrepel::geom_text_repel(\n    data = df |> filter(owned > 150000),\n    mapping = aes(label = name),\n    color = \"black\"\n    ) +\n  scale_x_continuous(limits = c(0, 10)) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"How do board game sales relate to online user ratings?\",\n    x = \"average user rating\",\n    y = \"number of\\ncopies owned\",\n    color = \"Category\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(\n    angle = 0,\n    vjust = .5\n  ))\n\n\n\n\nRelation board game sales and online user ratings"
  },
  {
    "objectID": "posts/02_TT/02_TT_202202.html#references",
    "href": "posts/02_TT/02_TT_202202.html#references",
    "title": "Research of Board Games",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/03_TT/03_TT_202202.html",
    "href": "posts/03_TT/03_TT_202202.html",
    "title": "Wrangling dates-time data",
    "section": "",
    "text": "The dataset for this Tidy Tuesday is about animal rescues! Alex Trinidad explores the temporal trends of animal rescues using lubridate package (Grolemund & Wickham, 2011) [@trinidad_nsc-r_nodate]"
  },
  {
    "objectID": "posts/03_TT/03_TT_202202.html#load-packages-and-data",
    "href": "posts/03_TT/03_TT_202202.html#load-packages-and-data",
    "title": "Wrangling dates-time data",
    "section": "Load packages and data",
    "text": "Load packages and data\nInstall TT package (if necessary)\ninstall.packages(\"tidytuesdayR\")\ninstall.packages(\"tidyverse\")\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidytuesdayR)\n\nDownload data.\n\nmydatalist <- tidytuesdayR::tt_load(\"2021-06-29\")\n\n\n    Downloading file 1 of 1: `animal_rescues.csv`\n\n\nData as tbl\n\nmydata <- mydatalist$animal_rescues"
  },
  {
    "objectID": "posts/03_TT/03_TT_202202.html#explore-the-data",
    "href": "posts/03_TT/03_TT_202202.html#explore-the-data",
    "title": "Wrangling dates-time data",
    "section": "Explore the data",
    "text": "Explore the data\n\nglimpse(mydata)\n\nRows: 7,544\nColumns: 31\n$ incident_number               <dbl> 139091, 275091, 2075091, 2872091, 355309~\n$ date_time_of_call             <chr> \"01/01/2009 03:01\", \"01/01/2009 08:51\", ~\n$ cal_year                      <dbl> 2009, 2009, 2009, 2009, 2009, 2009, 2009~\n$ fin_year                      <chr> \"2008/09\", \"2008/09\", \"2008/09\", \"2008/0~\n$ type_of_incident              <chr> \"Special Service\", \"Special Service\", \"S~\n$ pump_count                    <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", ~\n$ pump_hours_total              <chr> \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", ~\n$ hourly_notional_cost          <dbl> 255, 255, 255, 255, 255, 255, 255, 255, ~\n$ incident_notional_cost        <chr> \"510\", \"255\", \"255\", \"255\", \"255\", \"255\"~\n$ final_description             <chr> \"Redacted\", \"Redacted\", \"Redacted\", \"Red~\n$ animal_group_parent           <chr> \"Dog\", \"Fox\", \"Dog\", \"Horse\", \"Rabbit\", ~\n$ originof_call                 <chr> \"Person (land line)\", \"Person (land line~\n$ property_type                 <chr> \"House - single occupancy\", \"Railings\", ~\n$ property_category             <chr> \"Dwelling\", \"Outdoor Structure\", \"Outdoo~\n$ special_service_type_category <chr> \"Other animal assistance\", \"Other animal~\n$ special_service_type          <chr> \"Animal assistance involving livestock -~\n$ ward_code                     <chr> \"E05011467\", \"E05000169\", \"E05000558\", \"~\n$ ward                          <chr> \"Crystal Palace & Upper Norwood\", \"Woods~\n$ borough_code                  <chr> \"E09000008\", \"E09000008\", \"E09000029\", \"~\n$ borough                       <chr> \"Croydon\", \"Croydon\", \"Sutton\", \"Hilling~\n$ stn_ground_name               <chr> \"Norbury\", \"Woodside\", \"Wallington\", \"Ru~\n$ uprn                          <chr> \"NULL\", \"NULL\", \"NULL\", \"1.00021E+11\", \"~\n$ street                        <chr> \"Waddington Way\", \"Grasmere Road\", \"Mill~\n$ usrn                          <chr> \"20500146\", \"NULL\", \"NULL\", \"21401484\", ~\n$ postcode_district             <chr> \"SE19\", \"SE25\", \"SM5\", \"UB9\", \"RM3\", \"RM~\n$ easting_m                     <chr> \"NULL\", \"534785\", \"528041\", \"504689\", \"N~\n$ northing_m                    <chr> \"NULL\", \"167546\", \"164923\", \"190685\", \"N~\n$ easting_rounded               <dbl> 532350, 534750, 528050, 504650, 554650, ~\n$ northing_rounded              <dbl> 170050, 167550, 164950, 190650, 192350, ~\n$ latitude                      <chr> \"NULL\", \"51.39095371\", \"51.36894086\", \"5~\n$ longitude                     <chr> \"NULL\", \"-0.064166887\", \"-0.161985191\", ~\n\n\nDo we have missing data?\n\nsummary(mydata)\n\n incident_number     date_time_of_call     cal_year      fin_year        \n Min.   :     4149   Length:7544        Min.   :2009   Length:7544       \n 1st Qu.: 49306118   Class :character   1st Qu.:2012   Class :character  \n Median : 89438626   Mode  :character   Median :2015   Mode  :character  \n Mean   : 91854662                      Mean   :2015                     \n 3rd Qu.:131567118                      3rd Qu.:2018                     \n Max.   :233284091                      Max.   :2021                     \n NA's   :3478                                                            \n type_of_incident    pump_count        pump_hours_total   hourly_notional_cost\n Length:7544        Length:7544        Length:7544        Min.   :255.0       \n Class :character   Class :character   Class :character   1st Qu.:260.0       \n Mode  :character   Mode  :character   Mode  :character   Median :298.0       \n                                                          Mean   :301.3       \n                                                          3rd Qu.:333.0       \n                                                          Max.   :352.0       \n                                                                              \n incident_notional_cost final_description  animal_group_parent\n Length:7544            Length:7544        Length:7544        \n Class :character       Class :character   Class :character   \n Mode  :character       Mode  :character   Mode  :character   \n                                                              \n                                                              \n                                                              \n                                                              \n originof_call      property_type      property_category \n Length:7544        Length:7544        Length:7544       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n special_service_type_category special_service_type  ward_code        \n Length:7544                   Length:7544          Length:7544       \n Class :character              Class :character     Class :character  \n Mode  :character              Mode  :character     Mode  :character  \n                                                                      \n                                                                      \n                                                                      \n                                                                      \n     ward           borough_code         borough          stn_ground_name   \n Length:7544        Length:7544        Length:7544        Length:7544       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     uprn              street              usrn           postcode_district \n Length:7544        Length:7544        Length:7544        Length:7544       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  easting_m          northing_m        easting_rounded  northing_rounded\n Length:7544        Length:7544        Min.   :500050   Min.   :157050  \n Class :character   Class :character   1st Qu.:524750   1st Qu.:175150  \n Mode  :character   Mode  :character   Median :531650   Median :181250  \n                                       Mean   :531243   Mean   :180725  \n                                       3rd Qu.:537750   3rd Qu.:186750  \n                                       Max.   :571350   Max.   :200750  \n                                                                        \n   latitude          longitude        \n Length:7544        Length:7544       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n\nCreate a unique ID\n\nmydata <- mydata |> \n  arrange(cal_year) |> \n  mutate(uid = paste0(seq(1:n()), LETTERS, letters))\n\nAre there any duplicated?\n\ntable(duplicated(mydata$uid))\n\n\nFALSE \n 7544 \n\n\nSelect variables of interest.\n\nmydataselection <- mydata |> \n  select(uid, date_time_of_call, type_of_incident, animal_group_parent, borough_code)\n\nShow me the frequencies of different types of animal.\n\nmyfreq <- mydataselection |> \n  group_by(animal_group_parent) |> \n  summarise(freq = n()) |> \n  arrange(-freq)\nmyfreq\n\n# A tibble: 28 x 2\n   animal_group_parent               freq\n   <chr>                            <int>\n 1 Cat                               3649\n 2 Bird                              1530\n 3 Dog                               1194\n 4 Fox                                349\n 5 Horse                              193\n 6 Unknown - Domestic Animal Or Pet   191\n 7 Deer                               130\n 8 Unknown - Wild Animal               89\n 9 Squirrel                            65\n10 Unknown - Heavy Livestock Animal    49\n# ... with 18 more rows\n\n\nRemove unkonwn type of animals from the dataset.\n\nmydataselection <- mydataselection |> \n  filter(!grepl(\"Unknown\", animal_group_parent))\n\n\nmyfreq <- mydataselection |> \n  group_by(animal_group_parent) |> \n  summarise(freq = n()) |> \n  arrange(-freq)\n\nMerging the cat counts.\n\nmydataselection$animal_group_parent <- recode(mydataselection$animal_group_parent,\n                                              \"cat\" = \"Cat\")\n\nAnother way to do this (Nick van Doormaal suggestion).\n\nmydataselection$animal_group_parent <- tolower(mydataselection$animal_group_parent)"
  },
  {
    "objectID": "posts/03_TT/03_TT_202202.html#working-with-date-time-data",
    "href": "posts/03_TT/03_TT_202202.html#working-with-date-time-data",
    "title": "Wrangling dates-time data",
    "section": "Working with Date-Time Data",
    "text": "Working with Date-Time Data\nNow we are ready to work with Data-Time Data. We want to separate the date in year, month, day, hour….\nBut, what variable type is the date in our data set?\n\nglimpse(mydataselection)\n\nRows: 7,211\nColumns: 5\n$ uid                 <chr> \"1Aa\", \"2Bb\", \"3Cc\", \"4Dd\", \"5Ee\", \"7Gg\", \"8Hh\", \"~\n$ date_time_of_call   <chr> \"01/01/2009 03:01\", \"01/01/2009 08:51\", \"04/01/200~\n$ type_of_incident    <chr> \"Special Service\", \"Special Service\", \"Special Ser~\n$ animal_group_parent <chr> \"dog\", \"fox\", \"dog\", \"horse\", \"rabbit\", \"dog\", \"do~\n$ borough_code        <chr> \"E09000008\", \"E09000008\", \"E09000029\", \"E09000017\"~\n\n\nIf not “date” format, transform ir\n\nmydatadate <- mydataselection |> \n  mutate(datetime = lubridate::as_datetime(date_time_of_call, \n                                   format = \"%d/%m/%Y %H:%M\"))\n# # Non-lubridate Alternative\n# mydatadate <- mydataselection |> \n#   mutate(datetime = strptime(date_time_of_call,\n#                          format =\"%d/%m/%Y %H:%M\", \n#                          tz = \"Europe/London\"))\n# OlsonNames() # function for for the tz\n\nCreate separate variables for day, month, year, hour, minute, and date.\n\nmydatadate <- mydataselection |> \n  mutate(datetime = as_datetime(date_time_of_call,\n                            format =\"%d/%m/%Y %H:%M\"),\n         day = day(datetime),\n         month = month(datetime),\n         year = year(datetime),\n         hour = hour(datetime),\n         minute = minute(datetime),\n         date = as_date(datetime))\n\nhead(mydatadate[, 6:12])\n\n# A tibble: 6 x 7\n  datetime              day month  year  hour minute date      \n  <dttm>              <int> <dbl> <dbl> <int>  <int> <date>    \n1 2009-01-01 03:01:00     1     1  2009     3      1 2009-01-01\n2 2009-01-01 08:51:00     1     1  2009     8     51 2009-01-01\n3 2009-01-04 10:07:00     4     1  2009    10      7 2009-01-04\n4 2009-01-05 12:27:00     5     1  2009    12     27 2009-01-05\n5 2009-01-06 15:23:00     6     1  2009    15     23 2009-01-06\n6 2009-01-07 06:29:00     7     1  2009     6     29 2009-01-07\n\n\nHow many cases do we have now per day?\n\ncaseperday <- mydatadate |> \n  group_by(date) |> \n  summarise(resc_counts = n())\n\nPlot trends of cases\n\nggplot(data = caseperday, \n       aes(\n         x = date,\n         y = resc_counts\n       )) +\n  geom_line()\n\n\n\n\nTrends of cases\n\n\n\n\nAnd how many cases do we have per year?\n\nmydatadate |> \n  group_by(year) %>% \n  summarise(resc_counts = n()) |> \n  ggplot() +\n  aes(\n    x = year,\n    y = resc_counts\n  ) + \n  geom_line()\n\n\n\n\nHow many cases per year\n\n\n\n\nIs there a rescue every day?\n\nperday <- mydatadate |> \n          group_by(date) |> \n          summarise(resc_counts = n())\n\nHow many days are (more or less) in those years?\n\nlength(unique(mydatadate$year)) * 365\n\n[1] 4745\n\n\nHow can I know the days that are missing? Create for this a data set with all the days\n\ncompdates <- data.frame(date = c(seq(ymd('2009-01-01'), \n                                     ymd('2021-12-31'), by = '1 day')))\n\nHow can I know the days that are missing? Create for this a data set with all the days\n\ncompdates <- data.frame(date = c(seq(ymd('2009-01-01'), \n                                     ymd('2021-12-31'), by = '1 day')))\n\nSave missing dates\n\nmissingdates <- anti_join(compdates, perday)\n\nAdd missing dates to our data set.\nfulldates <- rbind(perday, missingdates) #\nThis will give an error because we need the same arguments We need the same arguments\n\nmissingdates <- missingdates %>% \n  mutate(resc_counts = vector(mode = \"numeric\", length = length(.)))\n\nAdd now the missing dates to our data set\n\nfulldates <- rbind(perday, missingdates)\n\nAre any date duplicated?\n\ntable(duplicated(fulldates$date))\n\n\nFALSE \n 4748 \n\n\nWim Bernasco’s suggestion instead of using anti_join() and rbind(), use left_join.\n\nfulldates <- left_join(compdates, perday, by = \"date\") %>% \n  replace(is.na(.), 0)\n\nSeparate the date ymd\n\nfulldates <- fulldates %>% \n  mutate(year = year(date),\n         month = month(date),\n         day = day(date))\n\nWhat week of the year did it happen?\n\nfulldates <- fulldates %>% \n  mutate(week = week(date))\n\nWhat day of the week did it happen?\n\nfulldates <- fulldates %>% \n  mutate(weekday = wday(date, label = TRUE))"
  },
  {
    "objectID": "posts/04_TT/04_TT_202203.html",
    "href": "posts/04_TT/04_TT_202203.html",
    "title": "Sankey diagrams and how to create",
    "section": "",
    "text": "In this meeting, Tim Verlaan explains what Sankey diagrams are, and how they can be created in R [@verlaan_nsc-rn_2022]"
  },
  {
    "objectID": "posts/04_TT/04_TT_202203.html#get-started",
    "href": "posts/04_TT/04_TT_202203.html#get-started",
    "title": "Sankey diagrams and how to create",
    "section": "Get started",
    "text": "Get started\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\n\n\nrelig_income\n\n# A tibble: 18 x 11\n   religion      `<$10k` $10-2~1 $20-3~2 $30-4~3 $40-5~4 $50-7~5 $75-1~6 $100-~7\n   <chr>           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Agnostic           27      34      60      81      76     137     122     109\n 2 Atheist            12      27      37      52      35      70      73      59\n 3 Buddhist           27      21      30      34      33      58      62      39\n 4 Catholic          418     617     732     670     638    1116     949     792\n 5 Don’t know/r~      15      14      15      11      10      35      21      17\n 6 Evangelical ~     575     869    1064     982     881    1486     949     723\n 7 Hindu               1       9       7       9      11      34      47      48\n 8 Historically~     228     244     236     238     197     223     131      81\n 9 Jehovah's Wi~      20      27      24      24      21      30      15      11\n10 Jewish             19      19      25      25      30      95      69      87\n11 Mainline Prot     289     495     619     655     651    1107     939     753\n12 Mormon             29      40      48      51      56     112      85      49\n13 Muslim              6       7       9      10       9      23      16       8\n14 Orthodox           13      17      23      32      32      47      38      42\n15 Other Christ~       9       7      11      13      13      14      18      14\n16 Other Faiths       20      33      40      46      49      63      46      40\n17 Other World ~       5       2       3       4       2       7       3       4\n18 Unaffiliated      217     299     374     365     341     528     407     321\n# ... with 2 more variables: `>150k` <dbl>, `Don't know/refused` <dbl>, and\n#   abbreviated variable names 1: `$10-20k`, 2: `$20-30k`, 3: `$30-40k`,\n#   4: `$40-50k`, 5: `$50-75k`, 6: `$75-100k`, 7: `$100-150k`\n\n\n\n?pivot_longer\n\n\npivot_longer(relig_income, !religion)\n\n# A tibble: 180 x 3\n   religion name               value\n   <chr>    <chr>              <dbl>\n 1 Agnostic <$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic >150k                 84\n10 Agnostic Don't know/refused    96\n# ... with 170 more rows\n\n\n\ndf <- relig_income |>\n  pivot_longer(!religion, names_to = 'income', values_to = 'count')\n\n\nbillboard\n\n# A tibble: 317 x 79\n   artist track date.ent~1   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8   wk9\n   <chr>  <chr> <date>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 2 Pac  Baby~ 2000-02-26    87    82    72    77    87    94    99    NA    NA\n 2 2Ge+h~ The ~ 2000-09-02    91    87    92    NA    NA    NA    NA    NA    NA\n 3 3 Doo~ Kryp~ 2000-04-08    81    70    68    67    66    57    54    53    51\n 4 3 Doo~ Loser 2000-10-21    76    76    72    69    67    65    55    59    62\n 5 504 B~ Wobb~ 2000-04-15    57    34    25    17    17    31    36    49    53\n 6 98^0   Give~ 2000-08-19    51    39    34    26    26    19     2     2     3\n 7 A*Tee~ Danc~ 2000-07-08    97    97    96    95   100    NA    NA    NA    NA\n 8 Aaliy~ I Do~ 2000-01-29    84    62    51    41    38    35    35    38    38\n 9 Aaliy~ Try ~ 2000-03-18    59    53    38    28    21    18    16    14    12\n10 Adams~ Open~ 2000-08-26    76    76    74    69    68    67    61    58    57\n# ... with 307 more rows, 67 more variables: wk10 <dbl>, wk11 <dbl>,\n#   wk12 <dbl>, wk13 <dbl>, wk14 <dbl>, wk15 <dbl>, wk16 <dbl>, wk17 <dbl>,\n#   wk18 <dbl>, wk19 <dbl>, wk20 <dbl>, wk21 <dbl>, wk22 <dbl>, wk23 <dbl>,\n#   wk24 <dbl>, wk25 <dbl>, wk26 <dbl>, wk27 <dbl>, wk28 <dbl>, wk29 <dbl>,\n#   wk30 <dbl>, wk31 <dbl>, wk32 <dbl>, wk33 <dbl>, wk34 <dbl>, wk35 <dbl>,\n#   wk36 <dbl>, wk37 <dbl>, wk38 <dbl>, wk39 <dbl>, wk40 <dbl>, wk41 <dbl>,\n#   wk42 <dbl>, wk43 <dbl>, wk44 <dbl>, wk45 <dbl>, wk46 <dbl>, wk47 <dbl>, ...\n\n\n\nbillboard |>\n  pivot_longer(\n    cols = starts_with('wk'),\n    values_drop_na = TRUE,\n    names_to = \"week\",\n    values_to = 'rank',\n    names_prefix = \"wk\",\n    names_transform = list(week = as.integer)\n  )\n\n# A tibble: 5,307 x 5\n   artist  track                   date.entered  week  rank\n   <chr>   <chr>                   <date>       <int> <dbl>\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26       1    87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26       2    82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26       3    72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26       4    77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26       5    87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26       6    94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26       7    99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02       1    91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02       2    87\n10 2Ge+her The Hardest Part Of ... 2000-09-02       3    92\n# ... with 5,297 more rows\n\n\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"davidsjoberg/ggsankey\")\nlibrary(ggsankey)\nlibrary(ggplot2)\n\n\n?mtcars\n\ndf <- mtcars |>\n  make_long(cyl, vs, am, gear, carb)\n\n\nggplot(df, aes(x = x,\n               node = node,\n               next_x = next_x,\n               next_node = next_node,\n               fill = factor(node),\n               label = node)) +\n  geom_sankey() +\n  geom_sankey_label() \n\n\n\n\nSankey graph\n\n\n\n\n\ndf1 <- mtcars |>\n  select(cyl, vs, am, gear, carb) |>\n  pivot_longer(everything()) |>\n  mutate(next_x = lead(.data$name),\n         next_node = lead(.data$value)\n  )"
  },
  {
    "objectID": "posts/welcome - kopie/index.html",
    "href": "posts/welcome - kopie/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome - kopie (2)/index.html",
    "href": "posts/welcome - kopie (2)/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome - kopie (3)/index.html",
    "href": "posts/welcome - kopie (3)/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome - kopie (4)/index.html",
    "href": "posts/welcome - kopie (4)/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/05_TT/05_TT_202203.html",
    "href": "posts/05_TT/05_TT_202203.html",
    "title": "Researching Spanish soccer data",
    "section": "",
    "text": "Sports in general and football (soccer) in particular provides a sheer endless source of open data that can be explored at various levels (player, team, match, competition) and from various perspectives. In this workshop Wim Bernasco uses open data about football borrowed from a GitHub repository maintained by James Curley (James P. Curley (2016). More information here [@bernasco_nsc-r_2022]\n[1^]Wim Bernasco, Franziska Yasrebi-de Kom provided fertile suggestions on code and comments"
  },
  {
    "objectID": "posts/05_TT/05_TT_202203.html#packages-and-data",
    "href": "posts/05_TT/05_TT_202203.html#packages-and-data",
    "title": "Researching Spanish soccer data",
    "section": "Packages and data",
    "text": "Packages and data\nInstall packages, the admin required to get going\n#install.packages('tidyverse')\n    #install.packages('lubridate')\n    #install.packages('devtools')\n    #library(devtools)\n    #install_github(\"jalapic/engsoccerdata\")\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(engsoccerdata)\n\nengsoccerdata is a package that includes data. Let us fist see what data are included (list the data in this package).\n\ndata(package=\"engsoccerdata\")  \n\nIf you like sports data, checkout for free datasets"
  },
  {
    "objectID": "posts/05_TT/05_TT_202203.html#exploration",
    "href": "posts/05_TT/05_TT_202203.html#exploration",
    "title": "Researching Spanish soccer data",
    "section": "Exploration",
    "text": "Exploration\nWhat type of dataset is spain? I hope a data.frame.\n\nspain |> class()\n\n[1] \"data.frame\"\n\n\nGreat, it is a data.frame\nWhat are the names of the variables. I hope they are self-explanatory.\n\nspain |> names()\n\n [1] \"Date\"    \"Season\"  \"home\"    \"visitor\" \"HT\"      \"FT\"      \"hgoal\"  \n [8] \"vgoal\"   \"tier\"    \"round\"   \"group\"   \"notes\"  \n\n\nLet us peek into the data.\n\nspain |> glimpse()\n\nRows: 23,915\nColumns: 12\n$ Date    <chr> \"1929-02-10\", \"1929-02-10\", \"1929-02-10\", \"1929-02-10\", \"1929-~\n$ Season  <dbl> 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 19~\n$ home    <chr> \"Arenas de Getxo\", \"Espanyol Barcelona\", \"Real Madrid\", \"Real ~\n$ visitor <chr> \"Atletico Madrid\", \"Real Union\", \"CE Europa\", \"Athletic Bilbao~\n$ HT      <chr> \"0-2\", \"1-0\", \"0-0\", \"1-1\", \"0-0\", \"0-1\", \"3-0\", \"0-3\", \"0-1\",~\n$ FT      <chr> \"2-3\", \"3-2\", \"5-0\", \"1-1\", \"0-2\", \"1-2\", \"9-0\", \"0-3\", \"3-1\",~\n$ hgoal   <dbl> 2, 3, 5, 1, 0, 1, 9, 0, 3, 5, 3, 3, 1, 0, 2, 1, 2, 3, 2, 2, 3,~\n$ vgoal   <dbl> 3, 2, 0, 1, 2, 2, 0, 3, 1, 2, 0, 1, 1, 4, 1, 2, 1, 0, 2, 0, 3,~\n$ tier    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ round   <chr> \"league\", \"league\", \"league\", \"league\", \"league\", \"league\", \"l~\n$ group   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\n$ notes   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA~\n\n\nYou should see something like this:\n# Rows: 25,435\n# Columns: 12\n# $ Date    <date> 1929-02-10, 1929-02-10, 1929-02-10, 1929-02-10, 1929-02-12, 1929-02-~\n# $ Season  <dbl> 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 1928, 192~\n# $ home    <chr> \"Arenas de Getxo\"\nThis is much more intuitive, but it only works interactively/ Try it out yourself, but do not include the View function in your script\nspain |> View()\nAlternatively, a quick look at the first 10 rows.\n\nspain |> \n  select(Date, home, visitor, FT) |> \n  head(n = 10)\n\n         Date               home            visitor  FT\n1  1929-02-10    Arenas de Getxo    Atletico Madrid 2-3\n2  1929-02-10 Espanyol Barcelona         Real Union 3-2\n3  1929-02-10        Real Madrid          CE Europa 5-0\n4  1929-02-10      Real Sociedad    Athletic Bilbao 1-1\n5  1929-02-12   Racing Santander       FC Barcelona 0-2\n6  1929-02-17       FC Barcelona        Real Madrid 1-2\n7  1929-02-17    Athletic Bilbao Espanyol Barcelona 9-0\n8  1929-02-17    Atletico Madrid      Real Sociedad 0-3\n9  1929-02-17         Real Union   Racing Santander 3-1\n10 1929-02-17          CE Europa    Arenas de Getxo 5-2\n\n\nLet us look at frequencies of some variables.\nHow many teams?\n\nspain |> count(home)\n\n                  home    n\n1           AD Almeria   34\n2             Albacete  135\n3      Arenas de Getxo   65\n4      Athletic Bilbao 1362\n5      Atletico Madrid 1288\n6      Atletico Tetuan   15\n7            Burgos CF  102\n8           CA Osasuna  639\n9             Cadiz CF  223\n10           CD Alaves  171\n11         CD Alcoyano   54\n12        CD Castellon  167\n13           CD Condal   15\n14          CD Leonesa   15\n15         CD Logrones  173\n16           CD Malaga  323\n17         CD Numancia   76\n18         CD Tenerife  247\n19           CE Europa   27\n20         CE Sabadell  213\n21          Celta Vigo  830\n22      CF Extremadura   40\n23          Cordoba CF  141\n24           CP Merida   40\n25 Deportivo La Coruna  746\n26            Elche CF  339\n27  Espanyol Barcelona 1294\n28        FC Barcelona 1362\n29           Getafe CF  228\n30           Gimnastic   58\n31          Granada CF  352\n32         Hercules CF  314\n33          Levante UD  201\n34           Malaga CF  285\n35       Pontevedra CF   90\n36    Racing Santander  713\n37      Rayo Vallecano  321\n38        RCD Mallorca  494\n39          Real Betis  845\n40         Real Burgos   57\n41           Real Jaen   45\n42         Real Madrid 1362\n43         Real Murcia  293\n44         Real Oviedo  596\n45       Real Sociedad 1132\n46          Real Union   36\n47     Real Valladolid  733\n48       Real Zaragoza  993\n49   Recreativo Huelva   93\n50       SD Compostela   80\n51            SD Eibar   38\n52          Sevilla FC 1185\n53      Sporting Gijon  710\n54          UD Almeria  114\n55       UD Las Palmas  529\n56        UD Salamanca  212\n57           UE Lleida   34\n58         Valencia CF 1313\n59       Villarreal CF  304\n60            Xerez CD   19\n\n\nFrequencies of variable-combinations\nHow many times did team A host team B? (Results not shown here, output=false, because of length)\n\nspain |> count(home, visitor)\n\nFrequencies of match outcomes\n\nspain |> count(hgoal, vgoal)\n\n   hgoal vgoal    n\n1      0     0 1811\n2      0     1 1320\n3      0     2  742\n4      0     3  299\n5      0     4  117\n6      0     5   30\n7      0     6   12\n8      0     7    4\n9      0     8    4\n10     1     0 2815\n11     1     1 2570\n12     1     2 1168\n13     1     3  431\n14     1     4  155\n15     1     5   58\n16     1     6   11\n17     1     7    2\n18     1     8    1\n19     2     0 2039\n20     2     1 2228\n21     2     2 1035\n22     2     3  357\n23     2     4  135\n24     2     5   35\n25     2     6    6\n26     2     7    2\n27     2     8    2\n28     3     0 1196\n29     3     1 1262\n30     3     2  652\n31     3     3  218\n32     3     4   47\n33     3     5   14\n34     3     6    6\n35     3     8    1\n36     4     0  643\n37     4     1  669\n38     4     2  325\n39     4     3  127\n40     4     4   28\n41     4     5    8\n42     4     6    2\n43     4     7    1\n44     5     0  268\n45     5     1  288\n46     5     2  149\n47     5     3   47\n48     5     4   22\n49     5     5    2\n50     5     6    1\n51     6     0  110\n52     6     1  116\n53     6     2   67\n54     6     3   19\n55     6     4    7\n56     6     6    1\n57     7     0   54\n58     7     1   48\n59     7     2   23\n60     7     3   11\n61     7     4    1\n62     7     5    1\n63     8     0   21\n64     8     1   20\n65     8     2   10\n66     8     3    6\n67     9     0   12\n68     9     1    5\n69     9     2    2\n70     9     3    1\n71     9     4    1\n72     9     5    1\n73    10     0    3\n74    10     1    5\n75    10     2    1\n76    10     3    1\n77    11     1    1\n78    11     2    1\n79    12     1    1\n\n\nSame, but using the combined FT variable.\n\nspain |> count(FT)\n\n     FT    n\n1   0-0 1811\n2   0-1 1320\n3   0-2  742\n4   0-3  299\n5   0-4  117\n6   0-5   30\n7   0-6   12\n8   0-7    4\n9   0-8    4\n10  1-0 2815\n11  1-1 2570\n12  1-2 1168\n13  1-3  431\n14  1-4  155\n15  1-5   58\n16  1-6   11\n17  1-7    2\n18  1-8    1\n19 10-0    3\n20 10-1    5\n21 10-2    1\n22 10-3    1\n23 11-1    1\n24 11-2    1\n25 12-1    1\n26  2-0 2039\n27  2-1 2228\n28  2-2 1035\n29  2-3  357\n30  2-4  135\n31  2-5   35\n32  2-6    6\n33  2-7    2\n34  2-8    2\n35  3-0 1196\n36  3-1 1262\n37  3-2  652\n38  3-3  218\n39  3-4   47\n40  3-5   14\n41  3-6    6\n42  3-8    1\n43  4-0  643\n44  4-1  669\n45  4-2  325\n46  4-3  127\n47  4-4   28\n48  4-5    8\n49  4-6    2\n50  4-7    1\n51  5-0  268\n52  5-1  288\n53  5-2  149\n54  5-3   47\n55  5-4   22\n56  5-5    2\n57  5-6    1\n58  6-0  110\n59  6-1  116\n60  6-2   67\n61  6-3   19\n62  6-4    7\n63  6-6    1\n64  7-0   54\n65  7-1   48\n66  7-2   23\n67  7-3   11\n68  7-4    1\n69  7-5    1\n70  8-0   21\n71  8-1   20\n72  8-2   10\n73  8-3    6\n74  9-0   12\n75  9-1    5\n76  9-2    2\n77  9-3    1\n78  9-4    1\n79  9-5    1\n\n\nFrequencies of a sum (total goals in match)\n\nspain |> count(hgoal + vgoal)\n\n   hgoal + vgoal    n\n1              0 1811\n2              1 4135\n3              2 5351\n4              3 4891\n5              4 3488\n6              5 2131\n7              6 1146\n8              7  543\n9              8  237\n10             9  113\n11            10   42\n12            11   17\n13            12    5\n14            13    4\n15            14    1\n\n\nFrequencies of function work as well (year of match date).\n\nspain |> count(year(Date))\n\n   year(Date)   n\n1        1929 115\n2        1930  84\n3        1931  91\n4        1932  95\n5        1933 110\n6        1934  75\n7        1935 150\n8        1936  84\n9        1939  30\n10       1940 186\n11       1941 139\n12       1942 182\n13       1943 175\n14       1944 189\n15       1945 182\n16       1946 189\n17       1947 175\n18       1948 189\n19       1949 189\n20       1950 205\n21       1951 240\n22       1952 216\n23       1953 256\n24       1954 248\n25       1955 216\n26       1956 264\n27       1957 232\n28       1958 240\n29       1959 240\n30       1960 232\n31       1961 264\n32       1962 216\n33       1963 239\n34       1964 249\n35       1965 240\n36       1966 233\n37       1967 239\n38       1968 248\n39       1969 248\n40       1970 232\n41       1971 246\n42       1972 324\n43       1973 306\n44       1974 288\n45       1975 315\n46       1976 306\n47       1977 297\n48       1978 306\n49       1979 306\n50       1980 332\n51       1981 306\n52       1982 297\n53       1983 310\n54       1984 320\n55       1985 306\n56       1986 324\n57       1987 366\n58       1988 379\n59       1989 401\n60       1990 368\n61       1991 371\n62       1992 380\n63       1993 391\n64       1994 370\n65       1995 428\n66       1996 451\n67       1997 454\n68       1998 350\n69       1999 401\n70       2000 369\n71       2001 400\n72       2002 350\n73       2003 401\n74       2004 379\n75       2005 381\n76       2006 369\n77       2007 391\n78       2008 370\n79       2009 370\n80       2010 390\n81       2011 380\n82       2012 390\n83       2013 380\n84       2014 369\n85       2015 390\n86       2016 211\n\n\nNote: this works because R knows that ‘Date’ is a date\nHow can you know that R knows? Either: (1) type ‘spain %>% glimpse’ and observe that the class of ‘Date’ is a \n(2) type ‘spain %>% pull(Date) %>% class()’ to obtain that information (pull returns a single variable from a dataframe)\n\nspain |> pull(Date) |> class()\n\n[1] \"character\"\n\n\nWhat is this ‘round’ variable?\n\nspain |> count(round)\n\n   round     n\n1 league 23825\n2 phase2    90\n\n\nApparently 90 matches (‘phase2’) are not regular La Liga matches. In subsequent analyses, we will not use these 90 matches. So we create a new dataframe excluding these 90 matches.\n\nspain_league <-\n  spain |>\n  filter(round==\"league\")\n\nWe will try to answer a couple of simple questions:\n1. Is it true that there are less goals today than in earlier days?\n2. Is the number of goals related to the season of the year?\n3. Is playing home really an advantage?\n4. If so, has this advantage changed over time?\n\nLess goals today?\n\nKey variables, just to check we have the variables we need and they look OK. Just list the first 10 cases\n\nspain_league |>\n  select(Date, home, visitor, hgoal, vgoal) |>\n  head(n=10)\n\n         Date               home            visitor hgoal vgoal\n1  1929-02-10    Arenas de Getxo    Atletico Madrid     2     3\n2  1929-02-10 Espanyol Barcelona         Real Union     3     2\n3  1929-02-10        Real Madrid          CE Europa     5     0\n4  1929-02-10      Real Sociedad    Athletic Bilbao     1     1\n5  1929-02-12   Racing Santander       FC Barcelona     0     2\n6  1929-02-17       FC Barcelona        Real Madrid     1     2\n7  1929-02-17    Athletic Bilbao Espanyol Barcelona     9     0\n8  1929-02-17    Atletico Madrid      Real Sociedad     0     3\n9  1929-02-17         Real Union   Racing Santander     3     1\n10 1929-02-17          CE Europa    Arenas de Getxo     5     2\n\n\nSame, but this time a random sample of rows.\n\nspain_league |>\n  select(Date, home, visitor, hgoal, vgoal) |>\n  slice_sample(n=10)\n\n         Date               home             visitor hgoal vgoal\n1  1983-12-14       RCD Mallorca       Real Sociedad     2     1\n2  1949-02-20 Espanyol Barcelona         Real Madrid     3     2\n3  1988-10-16    Atletico Madrid  Espanyol Barcelona     6     1\n4  2015-02-20          Getafe CF  Espanyol Barcelona     2     1\n5  1980-05-04       UD Salamanca          Sevilla FC     2     1\n6  1957-12-29         Sevilla FC         Valencia CF     0     1\n7  2005-04-16      Real Sociedad Deportivo La Coruna     1     0\n8  1970-03-08         Granada CF        RCD Mallorca     1     1\n9  1954-09-12    Real Valladolid       Real Sociedad     1     0\n10 1951-11-04        Valencia CF          Sevilla FC     2     0\n\n\nOnce more, number of goals in match\n\nspain_league |> count(hgoal + vgoal)\n\n   hgoal + vgoal    n\n1              0 1807\n2              1 4122\n3              2 5330\n4              3 4870\n5              4 3472\n6              5 2120\n7              6 1142\n8              7  543\n9              8  237\n10             9  113\n11            10   42\n12            11   17\n13            12    5\n14            13    4\n15            14    1\n\n\nHow did the number of goals per match develop over time?\n\nspain_league |>\n  # number of goals per match, and year of the match\n  mutate(goals = hgoal+vgoal,\n         year  = year(Date)\n  ) |>\n  ggplot() + \n  geom_point(aes(x=year, y=goals))\n\n\n\n\nYear and goals\n\n\n\n\nOeps, this was not what I had in mind. I need to aggregate first! The ‘group_by’ function does group the data per year, so that we can then use the summarize function to obtain the mean goals scored per match (mean_goals_pm) per year.\n\nspain_league |>\n  mutate(goals = hgoal + vgoal,\n         year  = year(Date)\n  ) |>\n  group_by(year) |>\n  summarize(mean_goals_pm = mean(goals)) |>\n  ggplot() + \n  geom_line(aes(x=year, y=mean_goals_pm))\n\n\n\n\nGoals grouped by year\n\n\n\n\nYes, there were more goals back in the old days (before 1950).\nFunny pattern. What about England? Do we have the same variables (yes!)\n\nengland |> names()\n\n [1] \"Date\"     \"Season\"   \"home\"     \"visitor\"  \"FT\"       \"hgoal\"   \n [7] \"vgoal\"    \"division\" \"tier\"     \"totgoal\"  \"goaldif\"  \"result\"  \n\n\n\nengland |>\n  # number of goals per match, and year of the match\n  mutate(goals = hgoal+vgoal,\n         year  = year(Date)\n  ) |>\n  group_by(year) |>\n  summarize(mean_goals_pm = mean(goals)) |>\n  ggplot() + \n  geom_line(aes(x=year, y=mean_goals_pm))\n\n\n\n\nMean goals grouped by year\n\n\n\n\nEngland is were the game was invented (they say), so they have a longer history going back to 1888. Let us now combine England and Spain.\n\nspain_series <-\n  spain_league |>\n  mutate(goals  = hgoal+vgoal,\n         year   = year(Date)\n  ) |>\n  group_by(year) |>\n  summarize(mean_goals_pm = mean(goals),\n            # Define a constant for Spain\n            country       = \"Spain\")\n\nWhat des this look like? Do it yourself: View(spain_series).\n\nengland_series <-\n  england |>\n  # number of goals per match, and year of the match\n  mutate(goals  = hgoal+vgoal,\n         year   = year(Date)\n  ) |>\n  group_by(year) |>\n  summarize(mean_goals_pm = mean(goals),\n            # Define a constant for England\n            country       = \"England\")\n\nStack both datasets on top of each other\n\nseries <- bind_rows(spain_series, england_series)\n\nTake a look yourself (note that England started 1888, Spain in 1929)\nseries |> \n  arrange(year, country) |> \n  View()\n  \nPlot development in Spain and England in the same graph\n\nseries |>\n  ggplot() +\n  geom_line(aes(x=year, y=mean_goals_pm, color=country))\n\n\n\n\nDevelopments in Spain and England\n\n\n\n\nBack to Spain\n\nIs the number of goals related to the season of the year? We use month of the year as a season indicator.\n\n\nspain_league |>\n  # number of goals per match, and year of the match\n  mutate(goals = hgoal+vgoal,\n         month  = month(Date)\n  ) |>\n  group_by(month) |>\n  summarize(mean_goals_pm = mean(goals)) |>\n  ggplot() + \n  geom_point(aes(x=month, y=mean_goals_pm)) +\n  scale_x_continuous(breaks=1:12)\n\n\n\n\nGoals related to the season\n\n\n\n\nLet us check how many we have in summer (June-August, numbers of games)\n\nspain_league |>\n  mutate(goals = hgoal+vgoal,\n         month  = month(Date)\n  ) |>\n  group_by(month) |>\n  # This is how we count nr of rows (matches) per month\n  summarize(number_of_matches = n()) |>\n  ggplot() + \n  geom_point(aes(x=month, y=number_of_matches)) +\n  # this gives us a scale nicely labeled 1..12\n  scale_x_continuous(breaks=1:12)\n\n\n\n\nNumber of games in summer\n\n\n\n\nLet us ignore June-August\n\nspain_league |>\n  mutate(goals = hgoal+vgoal,\n         month  = month(Date)\n  ) |>\n  # this removes months 6,7 and 8 (June, July, August)\n  #   the exclamation mark (!) means NOT , i.e.\n  # !x means the same as (x == FALSE)  \n  filter(!(month %in% c(6:8))) |>\n  group_by(month) |>\n  # This is how we count nr of rows (matches) per month\n  summarize(number_of_matches = n()) |>\n  ggplot() + \n  geom_point(aes(x=month, y=number_of_matches)) +\n  scale_x_continuous(breaks=1:12)\n\n\n\n\nNumber of matches without summer\n\n\n\n\nHowever, the differences are pretty small. This becomes clear when we set the scale of the Y axis\n\nspain_league |>\n  mutate(goals = hgoal+vgoal,\n         month  = month(Date) ) |>\n  filter(!(month %in% c(6:8))) |>\n  group_by(month) |>\n  summarize(mean_goals_pm = mean(goals)) |>\n  ggplot() + \n  geom_point(aes(x=month, y=mean_goals_pm)) +\n  scale_x_continuous(breaks=1:12) + \n  # Y axis range is between 0 and 3 goals\n  ylim(0,3)\n\n\n\n\nMean goals scaled\n\n\n\n\nOr as a bar chart\n\nspain_league |>\n  mutate(goals = hgoal+vgoal,\n         month  = month(Date) ) |>\n  filter(!(month %in% c(6:8))) |>\n  group_by(month) |>\n  summarize(mean_goals_pm = mean(goals)) |>\n  ggplot() + \n  # geom_col rather than geom_point\n  geom_col(aes(x=month, y=mean_goals_pm)) +\n  scale_x_continuous(breaks=1:12) + \n  # Y axis range is between 0 and 3 goals\n  ylim(0,3)\n\n\n\n\nBar chart of goals\n\n\n\n\nThe differences are negligible!\n\nIs playing home really an advantage?\n\nLet us create a custom version of the dataset for this analysis\n\nspain_extended <- spain_league |> \n  # Adding '-' in front of a variable means 'throw it away'\n  #   or, in other words, 'do not select it'. If you use the -\n  #   in select, variables not mentioned are retained.\n  select( -Season, -HT, -FT, -tier, -round, -group, -notes ) |>\n  mutate(goals = hgoal+vgoal,\n         year  = year(Date),\n         month  = month(Date),\n         # Continuous version : difference in goals\n         goals_difference = hgoal - vgoal,\n         # Discrete version: home wins, visitors win, equal split\n         result_discrete = case_when(goals_difference > 0  ~ \"Hosts wins\",\n                                     goals_difference < 0  ~ \"Visitors win\",\n                                     goals_difference == 0 ~ \"Equal split\") )\n\nFrequency of wins, losses and equal splits\n\nspain_extended |> count(result_discrete)\n\n  result_discrete     n\n1     Equal split  5644\n2      Hosts wins 13227\n3    Visitors win  4954\n\n\nIntermezzo (added after the workshop meeting)\nYou may be familiar with the ‘classic R’ table function to create this table (printed horizontally):\n\ntable(spain_extended$result_discrete)\n\n\n Equal split   Hosts wins Visitors win \n        5644        13227         4954 \n\n\nIf you prefer the classic table, the below will NOT work because there must always be a function after the pipe symbol (|>) as the pipe symbol means “whatever” is returned by the function before |> will become the first argument of the function after the |>.And ‘result_discrete’ is not a function, but an object.\nspain_extended |> result_discrete |> table() \nBut this will work, because pull is a function that returns the variable result_discrete as a vector):\n\nspain_extended |> pull(result_discrete) |> table()\n\n\n Equal split   Hosts wins Visitors win \n        5644        13227         4954 \n\n\nFrequency table with proportions.\n\nspain_extended |>\n  group_by(result_discrete) |>\n  summarize(frequency = n()) |>\n  mutate(proportion = frequency / sum(frequency))\n\n# A tibble: 3 x 3\n  result_discrete frequency proportion\n  <chr>               <int>      <dbl>\n1 Equal split          5644      0.237\n2 Hosts wins          13227      0.555\n3 Visitors win         4954      0.208\n\n\nSame thing, but now as a bar chart.\n\nspain_extended |>\n  group_by(result_discrete) |>\n  summarize(frequency = n()) |>\n  mutate(proportion = frequency / sum(frequency)) |>\n  ggplot() +\n  geom_col(aes(x=result_discrete, y=proportion), fill=\"blue\")\n\n\n\n\nProportions in a bar chart\n\n\n\n\nWhat does goal difference home-visitors look like\nKey descriptive variables.\n\nspain_extended |> \n  select(goals_difference) |> \n  summary()\n\n goals_difference\n Min.   :-8.000  \n 1st Qu.: 0.000  \n Median : 1.000  \n Mean   : 0.805  \n 3rd Qu.: 2.000  \n Max.   :11.000  \n\n\nMean: On average, home teams score .78 goals more per match than visitors.\nMedian: In half of the matches, the home team scores more than 1 goal more than the visitors\nLet us make a box plot\n\nspain_extended |> \n  ggplot() + \n  geom_boxplot(aes(x=goals_difference)) + \n  scale_x_continuous(breaks = seq(-10, 10,1))\n\n\n\n\nScores in a box plot\n\n\n\n\nHome teams clearly have an advantage!\n\nHas this advantage changed over time?\n\nProportions of matches won, lost, equal split\n\nspain_extended |>\n  group_by(year, result_discrete) |>\n  summarize(frequency = n()) |>\n  mutate(proportion = frequency / sum(frequency)) |>\n  ggplot() +\n  geom_line(aes(x=year, y=proportion, color=result_discrete))\n\n\n\n\nWon, lost and equal in lines\n\n\n\n\nGoal count differences between home team and visitors\n\nspain_extended |>\n  group_by(year) |>\n  summarize(home_visitors_difference = mean(goals_difference)) |>\n  ggplot() +\n  geom_line(aes(x=year, y=home_visitors_difference))\n\n\n\n\nDifferences between home team and visitors\n\n\n\n\nBoth graphs strongly suggest that home advantage decreased over the past century."
  },
  {
    "objectID": "posts/05_TT/05_TT_202203.html#references",
    "href": "posts/05_TT/05_TT_202203.html#references",
    "title": "Researching Spanish soccer data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html",
    "href": "posts/08_TT/08_TT_202205.html",
    "title": "Development of crime in population",
    "section": "",
    "text": "Below you find the materials Wim Bernasco prepared for the Tidy Tuesday workshop on May 24th 2022. Here is the original link to this workshop and the materials. You find links to two small datasets. One (crime.csv) contains the number of police recorded residential burglaries, bike thefts and assaults in the Netherlands per month between January 2012 and April 2022. The other (population.csv) contains the number of inhabitants of the Netherlands during the same period, per first day of the month. In the workshop he tried to make sense of long-term temporal trends in these crimes, and hopefully also make useful statements about how things changed when the COVID pandemic arrived around February 2020. He worked out eight steps in the analysis [@bernasco_nsc-r_2022-2].\nAcknowledgement: Franziska Yasrebi-de Kom corrected errors and provided fruitfull suggestions for improvement of the first draft."
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#start-up",
    "href": "posts/08_TT/08_TT_202205.html#start-up",
    "title": "Development of crime in population",
    "section": "Start-up",
    "text": "Start-up\nCreate a new folder on the computer where you run R, and store the two datafiles crime.csv and residents.csv in the folder.\nYou will normally load libraries here at the top of your script, but in this example I will load libraries at the point where I need their functionality. This shows when and why they are needed.\nIf your project is reasonably small and all files (data, scripts, and output files) can be stored in a single folder without creating confusion, setting a working folder is a good idea. All reading from and writing to disk will be done from and to this folder.\nsetwd(\"X:/YYY/ZZZ\") # for example: setwd(\"C:/Users/bernasco/crime_trends_NL\")\nNote: In this script I will use the new |> piping symbol. It is equivalent to %>% but has two advantages, which are\n- (1) it is shorter (2 instead of 3 symbols), and\n- (2) it does not require you to load a library, as it has been built into base R"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#read-both-files-from-disk-and-assign-each-a-name.",
    "href": "posts/08_TT/08_TT_202205.html#read-both-files-from-disk-and-assign-each-a-name.",
    "title": "Development of crime in population",
    "section": "Read both files from disk and assign each a name.",
    "text": "Read both files from disk and assign each a name.\nAny name will do, but I suggest “crime” and ” residents”. Read data using the read_csv function in package readr. read_csv has a few advantages over read.csv but the latter will also do.\nread_csv is in the readr library, so we load readr first. I added tidyverse also already.\n\nlibrary(readr)     # read_csv function\nlibrary(tidyverse)\n\nNote: in the live workshop we stumbled upon an error caused by our assumption that “crimes.csv” and “population.csv” had dates coded with dashes (like in “2022-05-24”) but my Excel version had written it with slashes (like in “2022/05/24”). Verify that after reading the files, their ‘date’ column has type ‘date’, not type ‘character’. You can use glimpse to verify this.\n\ncrimes <- read_csv(\"crimes.csv\")\n\npopulation <- read_csv(\"population.csv\")"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#explore-data",
    "href": "posts/08_TT/08_TT_202205.html#explore-data",
    "title": "Development of crime in population",
    "section": "Explore data",
    "text": "Explore data\nExplore the structure of the files. How many rows (observations), how many columns (variables), what are the variable names? What are their types: Are they integers, characters, dates, factors, .? keys? First explore the crime data.How many observations and how many variables (rows and columns)?\n\ncrimes |> dim()        # dim(crimes)\n\n[1] 372   3\n\n\nFor glimpse, slice-sample and count, we need library dplyr library(dplyr). Display the names and types of the variables, and show values for the first few observations column-wise. Note: verify that column date has type ‘date’\n\ncrimes |> glimpse()\n\nRows: 372\nColumns: 3\n$ date       <date> 2012-01-01, 2012-02-01, 2012-03-01, 2012-04-01, 2012-05-01~\n$ frequency  <dbl> 9787, 8237, 7944, 5955, 6198, 5493, 6168, 6481, 6102, 8178,~\n$ crime_type <chr> \"burglary\", \"burglary\", \"burglary\", \"burglary\", \"burglary\",~\n\n\nShow the first few cases row-wise\n\ncrimes |> head()\n\n# A tibble: 6 x 3\n  date       frequency crime_type\n  <date>         <dbl> <chr>     \n1 2012-01-01      9787 burglary  \n2 2012-02-01      8237 burglary  \n3 2012-03-01      7944 burglary  \n4 2012-04-01      5955 burglary  \n5 2012-05-01      6198 burglary  \n6 2012-06-01      5493 burglary  \n\n\nShow a random sample of 10 cases rather than the first ones. This will usually have more variation in values\n\ncrimes |> slice_sample(n=10) |> head()\n\n# A tibble: 6 x 3\n  date       frequency crime_type\n  <date>         <dbl> <chr>     \n1 2019-01-01      3281 assault   \n2 2014-03-01     10480 bike theft\n3 2019-07-01      7794 bike theft\n4 2014-09-01      4524 burglary  \n5 2021-01-01      2173 burglary  \n6 2019-11-01      3412 assault   \n\n\nFrequency table of crime types\n\ncrimes |> count(crime_type)\n\n# A tibble: 3 x 2\n  crime_type     n\n  <chr>      <int>\n1 assault      124\n2 bike theft   124\n3 burglary     124\n\n\nYou will see we have data for 124 months: 10 full years (2012-2021) + 4 months (Jan-April 2022)\nNext explore the population data\n\npopulation |> dim()\n\n[1] 123   2\n\n\nNote: verify that column date has type ‘date’.\n\npopulation |> glimpse()\n\nRows: 123\nColumns: 2\n$ date       <date> 2012-01-01, 2012-02-01, 2012-03-01, 2012-04-01, 2012-05-01~\n$ population <dbl> 16730348, 16731280, 16735690, 16737631, 16739764, 16742830,~\n\npopulation |> head()\n\n# A tibble: 6 x 2\n  date       population\n  <date>          <dbl>\n1 2012-01-01   16730348\n2 2012-02-01   16731280\n3 2012-03-01   16735690\n4 2012-04-01   16737631\n5 2012-05-01   16739764\n6 2012-06-01   16742830\n\npopulation |> slice_sample(n=10) |> head()\n\n# A tibble: 6 x 2\n  date       population\n  <date>          <dbl>\n1 2017-11-01   17172903\n2 2013-01-01   16779575\n3 2020-11-01   17472440\n4 2012-04-01   16737631\n5 2016-08-01   17022674\n6 2020-02-01   17413971"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#visualize-plot-population-and-crime-development",
    "href": "posts/08_TT/08_TT_202205.html#visualize-plot-population-and-crime-development",
    "title": "Development of crime in population",
    "section": "Visualize (plot) population and crime development",
    "text": "Visualize (plot) population and crime development\nVisualize (plot) the development of the population size of the Netherlands between January 2012 and April 2022, and do the same for the frequencies of residential burglary, bike theft and assault.The ggplot function is in the ggplot2 library.\n\nlibrary(ggplot2)\n\nPlot population as a scatterplot.\n\npopulation |> \n    ggplot() + \n    geom_point(mapping = aes(x = date, y = population))\n\n\n\n\nPopulation and crime development\n\n\n\n  # or as a line graph\n  population |> \n    ggplot() + \n    geom_line(mapping = aes(x = date, y = population))\n\n\n\n\nPopulation and crime development\n\n\n\n\nKeep it simple first. You can finetune later.Plot burglary frequencies\n\n##| warning: false\n#| echo: true\n#| label: burglary_freq\n#| fig-cap: \"Burglary frequencies\"\n#| cap-location: margin\n#| fig-width: 6\n#| fig-height: 4\n  crimes %>%\n    filter(crime_type == \"burglary\") |>\n    ggplot() + \n    geom_line(mapping = aes(x=date, y=frequency))\n\n\n\n\nYou should see a seasonal pattern with highs in Winter (nov-jan) and lows in summer. Plot bike theft\n\n crimes %>%\n    filter(crime_type == \"bike theft\") |>\n    ggplot() + \n    geom_line(mapping = aes(x = date, y = frequency)) \n\n\n\n\nBike theft\n\n\n\n\nYou should also see a seasonal pattern, but with peaks in autumn (dep-nov)\nPlot assault\n\ncrimes %>%\n    filter(crime_type == \"assault\") |>\n    ggplot() + \n    geom_line(mapping = aes(x = date, y = frequency))\n\n\n\n\nSeasonal pattern\n\n\n\n\nThe seasonal pattern for assault shows peaks in spring (apr-jun). Plot the three crime types in a single graph and distinguish them by color.\n\ncrimes |>\n    ggplot() + \n    geom_line(aes(x = date, y = frequency, color = crime_type)) \n\n\n\n\nThree crime types in one graph\n\n\n\n\nPlot them in separate panels.\n\ncrimes |>\n    ggplot() + \n    geom_line(aes(x = date, y = frequency)) +\n    # scales = \"free_y\" allows each panel to have its own\n    #  Y-axis scaling\n    facet_wrap(facets = \"crime_type\", scales = \"free_y\", ncol = 1)\n\n\n\n\nTypes in seperated panels\n\n\n\n\nI would not advise it, but you can combine panels and color (if you do, hide the legend to prevent redundancy)\n\ncrimes |>\n    ggplot() + \n    geom_line(aes(x = date, y = frequency, color = crime_type)) +\n    facet_wrap(facets = \"crime_type\", scales = \"free_y\", ncol = 1) +\n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#calculate-corrected-rate",
    "href": "posts/08_TT/08_TT_202205.html#calculate-corrected-rate",
    "title": "Development of crime in population",
    "section": "Calculate corrected rate",
    "text": "Calculate corrected rate\nCreate a residential burglary rate by relating number of residential burglaries to the size of the population, and think about how you can adjust for different months having a different numbers of days (28-31). To do this, you will need to merge (join) the “crime” dataframe with the “residents” dataframe by year and month.\nMerge crime and population by date (year and month).\n\n crime_population <- \n    # merge with crime type category labels\n    left_join(crimes, population, by = \"date\")\n\nTo calculate the number of days in the month of a date, you can use the day_in_month function which is in the lubridate library. Divide monthly crime frequency by number of days in the month to obtain the daily crime frequency, and divide by 100,000 to obtain daily crime rates per 100,000 population.\nFurther, I multiply the result by mean days per month to obtain the MONTHLY crime rate, but this is arbitrary (daily, weekly, of annual rates would be fine as well)\n\n  library(lubridate)\n  crime_population_corrected <-\n    crime_population |>\n    mutate(\n           # corrected for different month lengths\n           rate = frequency / days_in_month(date) * \n             (365.25/12) / (population / 100000))"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#visualize-development-of-corrected-crime-rates",
    "href": "posts/08_TT/08_TT_202205.html#visualize-development-of-corrected-crime-rates",
    "title": "Development of crime in population",
    "section": "Visualize development of corrected crime rates",
    "text": "Visualize development of corrected crime rates\n\ncrime_population_corrected |>\n    ggplot() + \n    geom_line(aes(x=date, y=rate, color=crime_type)) +\n    facet_wrap(facets = \"crime_type\", scales=\"free_y\", ncol=1)\n\n\n\n\nCorrected crime rates\n\n\n\n\nYou got this warning:\n#    Warning message:\n#    Removed 3 row(s) containing missing values    \nSo where are these three missing values?\n\ncrime_population_corrected |>\n    # select observations with NA (missing) values on 'rate'\n    filter(is.na(rate))\n\n# A tibble: 3 x 5\n  date       frequency crime_type population  rate\n  <date>         <dbl> <chr>           <dbl> <dbl>\n1 2022-04-01      1645 burglary           NA    NA\n2 2022-04-01      7096 bike theft         NA    NA\n3 2022-04-01      3416 assault            NA    NA\n\n\nAha! The population data of April 2022 were not yet online! This means we have valid data on both crime and population Jan 2012 to March 2022."
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#crime-before-and-after-covid",
    "href": "posts/08_TT/08_TT_202205.html#crime-before-and-after-covid",
    "title": "Development of crime in population",
    "section": "Crime before and after covid",
    "text": "Crime before and after covid\nWhat can we say about the development of crime since February 2020, relative to the developments between 2012 and 2020? How can you quantify this with the data at hand?\nLet’s look at burglary and start with making a plot that highlights the distinction between the periods before and during the pandemic.\nFirst we create a new variable/column that contains an indicator for whether the month is a pandemic month or not.\nDefine the date the pandemic started (a single number of the type ‘date’)\n\ncovid_start <- as_date(\"2020-02-01\")  \n\nNote: The expression: date >= covid_start returns TRUE (1) for ‘covid_start’ and all later dates, and returns FALSE (0) for all dates before ‘covid_start’. The labels parameter assigns ‘before’ to 0 and ‘during’ to 1\n\ncrime_population_corrected_covid <-  \n  crime_population_corrected |>\n  mutate(covid = factor(date >= covid_start , \n                        labels=c(\"before\", \"during\")))\n\nWe could just add a vertical line at the month that the pandemic started (February 2022)\n\ncrime_population_corrected_covid |>\n    filter(crime_type == \"burglary\") |>\n    ggplot() + \n    geom_line(aes(x=date, y=rate)) +\n    # add a red vertical line at given point on X-axis\n    geom_vline(xintercept = covid_start, color=\"red\")\n\n\n\n\nCrime during pandemic\n\n\n\n\nAlternatively, we could create a two-category variable indicating the before-after distinction, and then plot by color\n\ncrime_population_corrected_covid |>\n  filter(crime_type == \"burglary\") |>\n  ggplot() + \n  geom_line(aes(x=date, y=rate, color=covid))\n\n\n\n\nBefore-after distinction\n\n\n\n\nA combined graph for the three crime types\n\ncrime_population_corrected_covid |>\n  ggplot() + \n  geom_line(aes(x=date, y=rate, color=covid)) +\n  facet_wrap(facets = \"crime_type\", scales=\"free_y\", ncol=1)\n\n\n\n\nThree types combined\n\n\n\n\nNote that there is a ‘hole’ in de line graphs between January 2020 and February 2020. This is because we are actually drawing two separate line graphs here, one ending Jan 2022 and one starting Feb 2022. When we create a line graph, we drawing an individual point graph and then connect consecutive points with lines. So there really is nothing between Jan 2020 and Feb 2020.\nAs we think of time as continuous (by day, or even hour), we may want to create a ‘before covid’ line that continues to Feb 2020.One way to do this is to explicitly draw two line graphs in the same plot. As you see this quickly becomes complicated ….\nNote: I used “#F8766D” and “#00BFC4” to get the same colors that ggplot uses by default when there are two categories in a discrete variable\n\ncrime_population_corrected |>\n  mutate(precovid_rate = if_else(date <= as_date(\"2020-02-01\"),\n                                 rate, as.numeric(NA)),\n         postcovid_rate =if_else(date >= as_date(\"2020-02-01\"),\n                                 rate, as.numeric(NA))) |>\n  ggplot() + \n  geom_line(aes(x=date, y=precovid_rate), color=\"#F8766D\") +\n  geom_line(aes(x=date, y=postcovid_rate), color=\"#00BFC4\") +\n  xlab(\"rate\") +\n  facet_wrap(facets = \"crime_type\", scales=\"free_y\", ncol=1)  \n\n\n\n\nPre-covid rate\n\n\n\n\nNow let us quantify the trends. We first calculate annual rates as we are presently not interested in the seasonal variations.\n\nannual_rates_changes <-\n  crime_population_corrected |>\n  # create a variable indicating the year\n  mutate(year = year(date)) |>\n  # select only years 2012-2021 because we have not full 2022 data\n  filter(year < 2022) |>\n  # for each year and crime type, calculate annual crime rate\n  group_by(year, crime_type) |>\n  summarize(annual_rate = sum(rate)) |>\n  # Now calculate relative change: \n  #   By what proportion does this year's rate differ\n  #   from previous year's rate?\n  # We do this for each crime type separately\n  group_by(crime_type) |>\n  # sort by year \n  arrange(year) |>\n  # Copy last year's rate\n  mutate(lag_annual_rate = lag(annual_rate)) |>\n  # Relative change\n  # See, e.g. https://www.youtube.com/watch?v=muAkepkjpZI\n  mutate(annual_change = ((annual_rate - lag_annual_rate) / lag_annual_rate))\n\nJust to check that what you did generates the intended result: lag_annual_rate(2015) == annual_rate(2014) lag_annual_rate(2014) == annual_rate(2013) lag_annual_rate(2013) == annual_rate(2012) lag_annual_rate(2012) == NA (because annual_rate(2011) is unknown)\n\nannual_rates_changes |>\n  filter(crime_type == \"assault\") |>\n  head()\n\n# A tibble: 6 x 5\n# Groups:   crime_type [1]\n   year crime_type annual_rate lag_annual_rate annual_change\n  <dbl> <chr>            <dbl>           <dbl>         <dbl>\n1  2012 assault           336.             NA        NA     \n2  2013 assault           309.            336.       -0.0801\n3  2014 assault           295.            309.       -0.0456\n4  2015 assault           281.            295.       -0.0496\n5  2016 assault           271.            281.       -0.0335\n6  2017 assault           258.            271.       -0.0501\n\n\nPlot relative changes\n\nannual_rates_changes |>\n  ggplot() +\n  geom_line(aes(x=year, y=annual_rate, color=crime_type))\n\n\n\n\nRelative changes\n\n\n\n\nDefine the covid years\n\ncovid_years <- c(2020, 2021)\n\nCompare annual change before and during the pandemic per crime type\n\nannual_rates_changes %>%\n  mutate(covid = factor(year %in% covid_years, labels = c(\"before\", \"during\"))) |>\n  group_by(crime_type, covid) |>\n  summarize(mean_change = mean(annual_change, na.rm=TRUE))\n\n# A tibble: 6 x 3\n# Groups:   crime_type [3]\n  crime_type covid  mean_change\n  <chr>      <fct>        <dbl>\n1 assault    before     -0.0463\n2 assault    during     -0.0799\n3 bike theft before     -0.0587\n4 bike theft during     -0.0718\n5 burglary   before     -0.116 \n6 burglary   during     -0.233"
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#do-anything-else-with-the-data-that-you-find-fun-or-seems",
    "href": "posts/08_TT/08_TT_202205.html#do-anything-else-with-the-data-that-you-find-fun-or-seems",
    "title": "Development of crime in population",
    "section": "Do anything else with the data that you find fun or seems",
    "text": "Do anything else with the data that you find fun or seems\nTo make senseYou can see that there is an overall decline for each crime type (mean_change is always negative) and the decline is more pronounced (mean_change is more negative) during the pandemic than before the pandemic, in particular for assault and burglary.\nFor more rigorous statistical tests, we would need to dive into time series analysis."
  },
  {
    "objectID": "posts/08_TT/08_TT_202205.html#references",
    "href": "posts/08_TT/08_TT_202205.html#references",
    "title": "Development of crime in population",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html",
    "href": "posts/09_TT/09_TT_202211.html",
    "title": "Student mobility in Europe",
    "section": "",
    "text": "In this workshop, Wim Bernasco explores a Tidy Tuesday dataset about the European Union Erasmus+ student mobility program. This dataset was used in the main Tidy Tuesday of week 10 in 2022. For more information on this data, including the codebook, see the RForDataScience GitHub registry.\nIn this workshop, the focus is on exploring, analyzing, and maybe visualizing student streams between countries. Different descriptive questions will be answered [@bernasco_nsc-r_2022-6].\nHere you find the link to the NSC-R Tidy Tuesday page."
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html#get-started",
    "href": "posts/09_TT/09_TT_202211.html#get-started",
    "title": "Student mobility in Europe",
    "section": "Get started",
    "text": "Get started\nInstall uninstalled packages tidyverse, here and tidytuesdayR.\n\nif (! require(\"tidyverse\")) install.packages(\n  pkgs = \"tidyverse\", repos = \"http://cran.us.r-project.org\"\n)\nif (! require(\"here\")) install.packages(\n  pkgs = \"here\", repos = \"http://cran.us.r-project.org\"\n)\nif (! require(\"tidytuesdayR\")) install.packages(\n  pkgs = \"tidytuesdayR\", repos = \"http://cran.us.r-project.org\"\n)\nif (! require(\"broom\")) install.packages(\n  pkgs = \"broom\", repos = \"http://cran.us.r-project.org\"\n)\nif (! require(\"circlize\")) install.packages(\n  pkgs = \"circlize\", repos = \"http://cran.us.r-project.org\"\n)\nif (! require(\"igraph\")) install.packages(\n  pkgs = \"igraph\", repos = \"http://cran.us.r-project.org\"\n)\n\nLoad the required libraries\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(here)\nlibrary(tidytuesdayR)\nlibrary(circlize)\nlibrary(igraph)\n\nLoad the datafile For information see\n\n\n\n\n\n\nNote\n\n\n\nThe ‘participants’ field is a frequency weight!\n\n\nOpen the dataset\n\ntuesdata <- tt_load(2022, week = 10)\n\n\n    Downloading file 1 of 1: `erasmus.csv`\n\nerasmus <- tuesdata$erasmus\n\nThere are two additional ‘helper’ datafiles used used in this script: country_names.csv: The full names and EU-status of the countries adjacency.csv : Pairs of countries that are adjacent (share borders)\nI will address some of the following descriptive questions:\n- How many students studied abroad?\n- What are the top-10 receiving countries?\n- What are the top-10 sending countries?\n- Which are the 10 most frequent origin-destination country combinations?\n- Are reverse flows (the flow from A to B and the flow from B to A) correlated?\nThe following modeling questions will be answered:\n- How does total number of students from country A to country B depend on the total number of student from A and the total number of students from B?\n- Do adjacent countries attract more or less students than non-adjacent countries?\nIf time permits some visualization questions will be answered:\n- Are there some ways of visualizing mobility data?"
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html#exploration-and-preparation",
    "href": "posts/09_TT/09_TT_202211.html#exploration-and-preparation",
    "title": "Student mobility in Europe",
    "section": "Exploration and preparation",
    "text": "Exploration and preparation\nFirst we explore the data:\n\nerasmus |> names()\n\n [1] \"project_reference\"                   \"academic_year\"                      \n [3] \"mobility_start_month\"                \"mobility_end_month\"                 \n [5] \"mobility_duration\"                   \"activity_mob\"                       \n [7] \"field_of_education\"                  \"participant_nationality\"            \n [9] \"education_level\"                     \"participant_gender\"                 \n[11] \"participant_profile\"                 \"special_needs\"                      \n[13] \"fewer_opportunities\"                 \"group_leader\"                       \n[15] \"participant_age\"                     \"sending_country_code\"               \n[17] \"sending_city\"                        \"sending_organization\"               \n[19] \"sending_organisation_erasmus_code\"   \"receiving_country_code\"             \n[21] \"receiving_city\"                      \"receiving_organization\"             \n[23] \"receiving_organisation_erasmus_code\" \"participants\"                       \n\nerasmus |> glimpse()\n\nRows: 164,635\nColumns: 24\n$ project_reference                   <chr> \"2014-1-AT02-KA347-000139\", \"2014-~\n$ academic_year                       <chr> \"2014-2015\", \"2014-2015\", \"2014-20~\n$ mobility_start_month                <chr> \"2014-11\", \"2014-11\", \"2014-11\", \"~\n$ mobility_end_month                  <chr> \"2014-11\", \"2014-11\", \"2014-11\", \"~\n$ mobility_duration                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ activity_mob                        <chr> \"National youth meetings\", \"Nation~\n$ field_of_education                  <chr> \"? Unknown ?\", \"? Unknown ?\", \"? U~\n$ participant_nationality             <chr> \"AT\", \"AT\", \"AT\", \"AT\", \"AT\", \"AT\"~\n$ education_level                     <chr> \"??? - ? Unknown ?\", \"??? - ? Unkn~\n$ participant_gender                  <chr> \"Female\", \"Female\", \"Female\", \"Mal~\n$ participant_profile                 <chr> \"Learner\", \"Learner\", \"Learner\", \"~\n$ special_needs                       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"~\n$ fewer_opportunities                 <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\",~\n$ group_leader                        <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"~\n$ participant_age                     <dbl> 13, 14, 15, 14, 15, 15, 16, 17, 18~\n$ sending_country_code                <chr> \"AT\", \"AT\", \"AT\", \"AT\", \"AT\", \"AT\"~\n$ sending_city                        <chr> \"Dornbirn\", \"Dornbirn\", \"Dornbirn\"~\n$ sending_organization                <chr> \"bOJA - Bundesweites Netzwerk Offe~\n$ sending_organisation_erasmus_code   <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\",~\n$ receiving_country_code              <chr> \"AT\", \"AT\", \"AT\", \"AT\", \"AT\", \"AT\"~\n$ receiving_city                      <chr> \"Dornbirn\", \"Dornbirn\", \"Dornbirn\"~\n$ receiving_organization              <chr> \"bOJA - Bundesweites Netzwerk Offe~\n$ receiving_organisation_erasmus_code <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\",~\n$ participants                        <dbl> 2, 3, 3, 4, 2, 2, 1, 3, 1, 2, 1, 2~\n\nerasmus |> count(participants)\n\n# A tibble: 99 x 2\n   participants      n\n          <dbl>  <int>\n 1            1 118352\n 2            2  22804\n 3            3   8756\n 4            4   4642\n 5            5   2572\n 6            6   1775\n 7            7   1125\n 8            8    874\n 9            9    668\n10           10    502\n# ... with 89 more rows\n\n\nFirst check number of rows\n\nerasmus |> dim()\n\n[1] 164635     24\n\n\nNumber of rows after ‘expansion’. Expansion expands a frequency-weighted datafile to a regular file where each row represents a single student exchange trip.\n\nerasmus |> \n  uncount(participants) |>\n  dim()\n\n[1] 309751     23\n\n\nFrom which countries do participants come?\n\nerasmus |> \n  count(sending_country_code)\n\n# A tibble: 54 x 2\n   sending_country_code     n\n   <chr>                <int>\n 1 AL                     132\n 2 AM                     127\n 3 AT                    3484\n 4 AZ                      54\n 5 BA                      83\n 6 BE                    3279\n 7 BG                    4412\n 8 BY                      69\n 9 CY                    1886\n10 CZ                    6086\n# ... with 44 more rows\n\n\nTo which country do participants go?\n\nerasmus |>\n  count(receiving_country_code)\n\n# A tibble: 34 x 2\n   receiving_country_code     n\n   <chr>                  <int>\n 1 AT                      3451\n 2 BE                      4008\n 3 BG                      4120\n 4 CY                      1981\n 5 CZ                      6107\n 6 DE                     17000\n 7 DK                      1484\n 8 EE                      3637\n 9 EL                      2042\n10 ES                     11188\n# ... with 24 more rows\n\n\nAdd names to the codes. There are several options to do this. - Option 1: Explicit in-script recoding\n\nerasmus |>\n  mutate(sending_country_name =\n           case_when(sending_country_code == \"AT\" ~ \"Austria\",\n                     sending_country_code == \"BE\" ~ \"Belgium\",\n                     sending_country_code == \"BG\" ~ \"Bulgaria\",\n                     sending_country_code == \"CY\" ~ \"Cyprus\",\n                     sending_country_code == \"CZ\" ~ \"Czechia\",\n                     sending_country_code == \"DE\" ~ \"Germany\",\n                     sending_country_code == \"DK\" ~ \"Denkmark\"\n                     # .........\n           )) |>\n  count(sending_country_name)\n\n# A tibble: 8 x 2\n  sending_country_name      n\n  <chr>                 <int>\n1 Austria                3484\n2 Belgium                3279\n3 Bulgaria               4412\n4 Cyprus                 1886\n5 Czechia                6086\n6 Denkmark               1426\n7 Germany               17155\n8 <NA>                 126907\n\n\n\nOption 2: Join with look-up table in a separate CSV file\n\n\ncountry_labels <- read_csv(here(\"C:/Users/Gebruiker/Desktop/TidyTuesday/TT_20221115/country_names.csv\"))\n\n\nerasmus |>\n  left_join(country_labels, \n            by=c(\"receiving_country_code\" = \"country_code\")) |>\n  rename(receiving_country_name = country_name,\n         receiving_country_status = country_status)\n\n# A tibble: 164,635 x 26\n   project_ref~1 acade~2 mobil~3 mobil~4 mobil~5 activ~6 field~7 parti~8 educa~9\n   <chr>         <chr>   <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>  \n 1 2014-1-AT02-~ 2014-2~ 2014-11 2014-11       1 Nation~ ? Unkn~ AT      ??? - ~\n 2 2014-1-AT02-~ 2014-2~ 2014-11 2014-11       1 Nation~ ? Unkn~ AT      ??? - ~\n 3 2014-1-AT02-~ 2014-2~ 2014-11 2014-11       1 Nation~ ? Unkn~ AT      ??? - ~\n 4 2014-1-AT02-~ 2014-2~ 2014-11 2014-11       1 Nation~ ? Unkn~ AT      ??? - ~\n 5 2014-1-AT02-~ 2014-2~ 2014-11 2014-11       1 Nation~ ? Unkn~ AT      ??? - ~\n 6 2014-1-AT02-~ 2014-2~ 2014-12 2014-12       1 Nation~ ? Unkn~ AT      ??? - ~\n 7 2014-1-AT02-~ 2014-2~ 2014-12 2014-12       1 Nation~ ? Unkn~ AT      ??? - ~\n 8 2014-1-AT02-~ 2014-2~ 2014-12 2014-12       1 Nation~ ? Unkn~ AT      ??? - ~\n 9 2014-1-AT02-~ 2014-2~ 2014-12 2014-12       1 Nation~ ? Unkn~ AT      ??? - ~\n10 2014-1-AT02-~ 2014-2~ 2014-12 2014-12       1 Nation~ ? Unkn~ AT      ??? - ~\n# ... with 164,625 more rows, 17 more variables: participant_gender <chr>,\n#   participant_profile <chr>, special_needs <chr>, fewer_opportunities <chr>,\n#   group_leader <chr>, participant_age <dbl>, sending_country_code <chr>,\n#   sending_city <chr>, sending_organization <chr>,\n#   sending_organisation_erasmus_code <chr>, receiving_country_code <chr>,\n#   receiving_city <chr>, receiving_organization <chr>,\n#   receiving_organisation_erasmus_code <chr>, participants <dbl>, ...\n\n\nCombining some of the above transformations we created a clean file\n\nlabeled_erasmus_full <- \n  erasmus |>\n  # Keep only a subset of columns/variables\n  select(sending_country_code, receiving_country_code, \n         participant_gender, academic_year, activity_mob, participants) |>\n  # insert names of receiving countries by linking to country codes\n  left_join(country_labels, \n            by=c(\"receiving_country_code\" = \"country_code\")) |>\n  # make sure the column names are clear \n  rename(receiving_country_name = country_name,\n         receiving_country_status = country_status) |> \n  # insert names of sending countries by linking to country codes\n  left_join(country_labels, \n            by=c(\"sending_country_code\" = \"country_code\")) |>\n  # make sure the column names are clear \n  rename(sending_country_name = country_name,\n         sending_country_status = country_status) |> \n  # exclude countries outside EU and with no affiliation to EU\n  filter(sending_country_status %in% c(\"EU\", \"EFTA\", \"UK\", \"Candidate\"),\n         receiving_country_status %in% c(\"EU\", \"EFTA\", \"UK\", \"Candidate\")) |>\n  # exclude the (many!) within-country exchanges\n  filter(sending_country_code != receiving_country_code) |>\n  # Only international mobility program\n  filter(activity_mob == \"Transnational youth meetings\") |>\n  # Every row becomes an individual international student trip \n  uncount(participants)"
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html#descriptive-questions",
    "href": "posts/09_TT/09_TT_202211.html#descriptive-questions",
    "title": "Student mobility in Europe",
    "section": "Descriptive questions",
    "text": "Descriptive questions\nHow many students are there?\n\nlabeled_erasmus_full |>\n  dim()\n\n[1] 15693     9\n\n\nWhere did they come from?\n\nlabeled_erasmus_full |> \n  count(sending_country_name) |>\n  print(n=Inf)\n\n# A tibble: 36 x 2\n   sending_country_name     n\n   <chr>                <int>\n 1 Albania                169\n 2 Austria                565\n 3 Belgium                662\n 4 Bulgaria               526\n 5 Croatia                323\n 6 Cyprus                 121\n 7 Czechia                531\n 8 Denmark                296\n 9 Estonia                292\n10 Finland                307\n11 France                 622\n12 Germany               1314\n13 Greece                 483\n14 Hungary                473\n15 Iceland                 25\n16 Ireland                225\n17 Italy                 1098\n18 Latvia                 386\n19 Liechtenstein            6\n20 Lithuania              411\n21 Luxembourg             166\n22 Malta                   58\n23 Montenegro              27\n24 Netherlands            630\n25 North Macedonia        185\n26 Norway                 133\n27 Poland                 837\n28 Portugal               468\n29 Romania                745\n30 Serbia                 343\n31 Slovakia               370\n32 Slovenia               268\n33 Spain                  819\n34 Sweden                 495\n35 Turkey                 553\n36 United Kingdom         761\n\n\nVisualization as a bar graph\n\nlabeled_erasmus_full |> \n  count(sending_country_name) |>  \n  ggplot() + \n  geom_col(aes(x=sending_country_name, y=n))\n\n\n\n\nWhere do students come from, horizontal\n\n\n\n\nWith vertical labels labels\n\nlabeled_erasmus_full |> \n  count(sending_country_name) |>  \n  ggplot() + \n  geom_col(aes(x=sending_country_name, y=n)) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nWhere do students come from, vertical\n\n\n\n\nOrder by frequency.\n\nlabeled_erasmus_full |> \n  count(sending_country_code) |>  \n  arrange(-n) |>\n    # Converting character variable to factor variable\n    mutate(sending_country_code=factor(sending_country_code,\n                                     levels = unique(sending_country_code),\n                                     ordered = T)) |>\n  ggplot() + \n  geom_col(aes(x=sending_country_code, y=n)) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nWhere do students come from, ordered\n\n\n\n\nWhere did they go to?\n\nlabeled_erasmus_full |> \n  count(receiving_country_name) |>\n  arrange(-n) |>\n  print(n=Inf)\n\n# A tibble: 31 x 2\n   receiving_country_name     n\n   <chr>                  <int>\n 1 France                  2403\n 2 Belgium                 1428\n 3 Spain                   1204\n 4 Germany                 1095\n 5 Italy                   1017\n 6 Netherlands              765\n 7 Luxembourg               607\n 8 Estonia                  524\n 9 Czechia                  518\n10 Lithuania                495\n11 Austria                  485\n12 United Kingdom           483\n13 Greece                   444\n14 Romania                  423\n15 Turkey                   371\n16 Slovenia                 355\n17 Croatia                  343\n18 Denmark                  340\n19 Poland                   340\n20 Portugal                 296\n21 Norway                   286\n22 Sweden                   250\n23 Cyprus                   214\n24 Ireland                  186\n25 Latvia                   165\n26 Malta                    163\n27 Bulgaria                 149\n28 Hungary                  116\n29 North Macedonia          111\n30 Slovakia                  67\n31 Finland                   50\n\n\n\nlabeled_erasmus_full |> \n  count(receiving_country_name) |>\n  arrange(-n) |>\n  print(n=Inf)\n\n# A tibble: 31 x 2\n   receiving_country_name     n\n   <chr>                  <int>\n 1 France                  2403\n 2 Belgium                 1428\n 3 Spain                   1204\n 4 Germany                 1095\n 5 Italy                   1017\n 6 Netherlands              765\n 7 Luxembourg               607\n 8 Estonia                  524\n 9 Czechia                  518\n10 Lithuania                495\n11 Austria                  485\n12 United Kingdom           483\n13 Greece                   444\n14 Romania                  423\n15 Turkey                   371\n16 Slovenia                 355\n17 Croatia                  343\n18 Denmark                  340\n19 Poland                   340\n20 Portugal                 296\n21 Norway                   286\n22 Sweden                   250\n23 Cyprus                   214\n24 Ireland                  186\n25 Latvia                   165\n26 Malta                    163\n27 Bulgaria                 149\n28 Hungary                  116\n29 North Macedonia          111\n30 Slovakia                  67\n31 Finland                   50\n\n\nLet us visualise this also, vertical and ordered.\n\nlabeled_erasmus_full |> \n  count(receiving_country_code) |>  \n  arrange(-n) |>\n  mutate(receiving_country_code=factor(receiving_country_code,\n                                       levels = unique(receiving_country_code),\n                                       ordered = T)) |>\n  ggplot() + \n  geom_col(aes(x=receiving_country_code, y=n)) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nHow many students do countries receive, ordered\n\n\n\n\nTop 10 Where did they go to.\n\nlabeled_erasmus_full |> \n  count(receiving_country_name) |>\n  arrange(-n) |> \n  head(n=10)\n\n# A tibble: 10 x 2\n   receiving_country_name     n\n   <chr>                  <int>\n 1 France                  2403\n 2 Belgium                 1428\n 3 Spain                   1204\n 4 Germany                 1095\n 5 Italy                   1017\n 6 Netherlands              765\n 7 Luxembourg               607\n 8 Estonia                  524\n 9 Czechia                  518\n10 Lithuania                495\n\n\nTop 10 Where did they come from\n\nlabeled_erasmus_full |> \n  count(sending_country_name) |>\n  arrange(-n) |> \n  head(n=10)\n\n# A tibble: 10 x 2\n   sending_country_name     n\n   <chr>                <int>\n 1 Germany               1314\n 2 Italy                 1098\n 3 Poland                 837\n 4 Spain                  819\n 5 United Kingdom         761\n 6 Romania                745\n 7 Belgium                662\n 8 Netherlands            630\n 9 France                 622\n10 Austria                565\n\n\nTop 10 origin-destination combinations\n\nlabeled_erasmus_full |> \n  count(sending_country_name, receiving_country_name) |>\n  arrange(-n) |> \n  head(n=10)\n\n# A tibble: 10 x 3\n   sending_country_name receiving_country_name     n\n   <chr>                <chr>                  <int>\n 1 Germany              France                   305\n 2 Italy                France                   268\n 3 Germany              Belgium                  235\n 4 Belgium              France                   198\n 5 Italy                Spain                    170\n 6 Spain                France                   149\n 7 United Kingdom       France                   131\n 8 Spain                Italy                    123\n 9 Poland               France                   122\n10 France               Belgium                  120\n\n\n# Intermezzo \n(Cartesian product = cross-product = all combinations) \n\nTiny dataset of team members\n\n::: {.cell}\n\n```{.r .cell-code}\nteam_members <- tibble(name = c( \"Alex\", \"Asier\", \"Franziska\", \"Sam\", \"Wim\")) \n# Tiny datset of available days\nsessions <- tibble(day = c(\"Monday\", \"Tuesday\", \"Thursday\"))\n:::\nMake all combinations of team members and available days\n\nfull_join(team_members, sessions, by = as.character())\n\n# A tibble: 15 x 2\n   name      day     \n   <chr>     <chr>   \n 1 Alex      Monday  \n 2 Alex      Tuesday \n 3 Alex      Thursday\n 4 Asier     Monday  \n 5 Asier     Tuesday \n 6 Asier     Thursday\n 7 Franziska Monday  \n 8 Franziska Tuesday \n 9 Franziska Thursday\n10 Sam       Monday  \n11 Sam       Tuesday \n12 Sam       Thursday\n13 Wim       Monday  \n14 Wim       Tuesday \n15 Wim       Thursday\n\n\nSame results, but more transparent code.\n\n\nthanks to Nick van Doormaal for this suggestion\n\nexpand_grid(team_members, sessions)\n\n# A tibble: 15 x 2\n   name      day     \n   <chr>     <chr>   \n 1 Alex      Monday  \n 2 Alex      Tuesday \n 3 Alex      Thursday\n 4 Asier     Monday  \n 5 Asier     Tuesday \n 6 Asier     Thursday\n 7 Franziska Monday  \n 8 Franziska Tuesday \n 9 Franziska Thursday\n10 Sam       Monday  \n11 Sam       Tuesday \n12 Sam       Thursday\n13 Wim       Monday  \n14 Wim       Tuesday \n15 Wim       Thursday\n\nfull_join(team_members, team_members, by = as.character()) |>\n  filter(name.x != name.y)\n\n# A tibble: 20 x 2\n   name.x    name.y   \n   <chr>     <chr>    \n 1 Alex      Asier    \n 2 Alex      Franziska\n 3 Alex      Sam      \n 4 Alex      Wim      \n 5 Asier     Alex     \n 6 Asier     Franziska\n 7 Asier     Sam      \n 8 Asier     Wim      \n 9 Franziska Alex     \n10 Franziska Asier    \n11 Franziska Sam      \n12 Franziska Wim      \n13 Sam       Alex     \n14 Sam       Asier    \n15 Sam       Franziska\n16 Sam       Wim      \n17 Wim       Alex     \n18 Wim       Asier    \n19 Wim       Franziska\n20 Wim       Sam      \n\n\n\nCreate all possible combinations of sending and receiving country names.\n\n::: {.cell}\n\n```{.r .cell-code}\npossible_mobility_names <- \n  full_join(country_labels, country_labels,\n            by = as.character()) |>\n  select(sending_country_name   = country_name.x,\n         receiving_country_name = country_name.y,\n         sending_country_code   = country_code.x,\n         receiving_country_code = country_code.y) |>\n  filter(sending_country_name != receiving_country_name)\n:::\nErasmus student mobility flows including zero flows\n\nflows_erasmus_full_zeros <-\n  labeled_erasmus_full |> \n  group_by(sending_country_name, receiving_country_name) |>\n  # Origin, destination, count\n  count() |>\n  rename(exchanges = n) |>\n  # join with all combinations to include zero-flow pairs\n  right_join(possible_mobility_names) |> \n  # change the NAs (= zero-flow) into 0\n  replace_na(list(exchanges=0))\n\nNumber of exchanges frequencies reversed: 581 zero flows, 37 1-person flow\n\nflows_erasmus_full_zeros |>\n  ungroup() |>\n  count(exchanges) |>\n  arrange(exchanges) |>\n  print(n=40)\n\n# A tibble: 98 x 2\n   exchanges     n\n       <int> <int>\n 1         0   581\n 2         1    37\n 3         2    37\n 4         3    48\n 5         4    35\n 6         5    42\n 7         6    30\n 8         7    32\n 9         8    29\n10         9    19\n11        10    28\n12        11    20\n13        12    27\n14        13    14\n15        14    18\n16        15    20\n17        16    15\n18        17    15\n19        18    13\n20        19    15\n21        20    16\n22        21    10\n23        22    13\n24        23    14\n25        24     9\n26        25    13\n27        26     7\n28        27     7\n29        28     7\n30        29    13\n31        30     8\n32        31     9\n33        32     4\n34        33     7\n35        34     4\n36        35     4\n37        36     6\n38        37     6\n39        38     4\n40        39     3\n# ... with 58 more rows\n\n\nLet us make a histogram of this distribution\n\nflows_erasmus_full_zeros |>\n  ggplot() +\n  geom_histogram(aes(x=exchanges), binwidth = 5)\n\n\n\n\nHistogram of student mobility flow\n\n\n\n\n\nreverse_flows_erasmus_full_zeros <-\n  flows_erasmus_full_zeros |>\n  rename(sending_country_name = receiving_country_name,\n         receiving_country_name = sending_country_name,\n         reverse_exchanges = exchanges)\n\n\nfull_join(flows_erasmus_full_zeros, reverse_flows_erasmus_full_zeros,\n          by = (c(\"sending_country_name\", \"receiving_country_name\"))) |> \n  ungroup() |>\n  select(exchanges, reverse_exchanges) |>\n  cor() |> \n  as_tibble()\n\n# A tibble: 2 x 2\n  exchanges reverse_exchanges\n      <dbl>             <dbl>\n1     1                 0.463\n2     0.463             1    \n\n\nThis scatterplot is by design symetric in the diagonal\n\nfull_join(flows_erasmus_full_zeros, reverse_flows_erasmus_full_zeros,\n          by = (c(\"sending_country_name\", \"receiving_country_name\"))) |> \n  ungroup() |>\n  select(exchanges, reverse_exchanges) |>\n  ggplot() +\n  geom_point(aes(y=exchanges, x=reverse_exchanges) )\n\n\n\n\nRelation between exchanges"
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html#modeling",
    "href": "posts/09_TT/09_TT_202211.html#modeling",
    "title": "Student mobility in Europe",
    "section": "Modeling",
    "text": "Modeling\nRead the neighbor relations between countries\n\nadjacency <- read_csv(here(\"C:/Users/Gebruiker/Desktop/TidyTuesday/TT_20221115/adjacency.csv\")) |>\n  rename(sending_country_name = country_name,\n         receiving_country_name = neighbor) |>\n  # mark the rows that indicate shared borders\n  mutate(adjacent = 1) |>\n  # merge with the data that include all possible mobility streams\n  right_join(possible_mobility_names,\n             by=c(\"sending_country_name\",\"receiving_country_name\")) |>\n  # Set non-adjacent to 0\n  mutate(adjacent = replace_na(adjacent, 0)) \n\n\nmodel_data <- \n  flows_erasmus_full_zeros |> \n  inner_join(adjacency, \n            by = c(\"sending_country_name\",\"receiving_country_name\")) |>  \n  group_by(sending_country_name) |>\n  # total outflow from country\n  mutate(outflow = sum(exchanges)) |>\n  group_by(receiving_country_name) |>\n  # total inflow into country\n  mutate(inflow = sum(exchanges)) \n\nNumber of student exchanges from A to B as a function of total numbers of outgoing students from A and of total numbers of visiting students in B\n\nmodel_01 <- lm(formula = exchanges ~  inflow + outflow ,          , \n               data = model_data)\ntidy(model_01)\n\n# A tibble: 3 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) -12.2      0.942       -13.0 2.49e- 36\n2 inflow        0.0281   0.00100      27.9 1.63e-135\n3 outflow       0.0285   0.00161      17.7 2.55e- 63\n\nglance(model_01)\n\n# A tibble: 1 x 12\n  r.squared adj.r.s~1 sigma stati~2   p.value    df logLik    AIC    BIC devia~3\n      <dbl>     <dbl> <dbl>   <dbl>     <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.448     0.447  17.5    539. 2.98e-172     2 -5703. 11415. 11436. 408538.\n# ... with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names 1: adj.r.squared, 2: statistic, 3: deviance\n\n\nAdd adjacency\n\nmodel_02 <- lm(formula = exchanges ~  inflow + outflow + adjacent, \n               data = model_data)\ntidy(model_02)\n\n# A tibble: 4 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) -12.9     0.881        -14.7 2.03e- 45\n2 inflow        0.0274  0.000939      29.1 1.27e-144\n3 outflow       0.0265  0.00151       17.5 3.93e- 62\n4 adjacent     23.0     1.64          14.0 9.76e- 42\n\nglance(model_02)\n\n# A tibble: 1 x 12\n  r.squared adj.r.s~1 sigma stati~2   p.value    df logLik    AIC    BIC devia~3\n      <dbl>     <dbl> <dbl>   <dbl>     <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.519     0.518  16.4    478. 1.36e-210     3 -5612. 11233. 11259. 355879.\n# ... with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names 1: adj.r.squared, 2: statistic, 3: deviance\n\n\nStudents appear to fancy visiting nearby countries abroad!"
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html#visualization",
    "href": "posts/09_TT/09_TT_202211.html#visualization",
    "title": "Student mobility in Europe",
    "section": "Visualization",
    "text": "Visualization\nVisualization 1: Chord diagram (using library circlize)\nFull country names\n\nflows_erasmus_full_zeros |> \n  filter(exchanges > 0) |>\n  arrange(-exchanges) |> \n  chordDiagram()\n\n\n\n\nChord diagram\n\n\n\n\nUse the 2-letter abbreviations of countries here.\n\nflows_erasmus_full_zeros |> \n  filter(exchanges > 0) |>\n  arrange(-exchanges) |> \n  # without ungroup() the .._country_name columns will\n  # be retained\n  ungroup() |>\n  select(sending_country_code, receiving_country_code,\n         exchanges) |>\n  chordDiagram()\n\n\n\n\nChord diagram with 2-letter abbreviations\n\n\n\n\nOnly flows over 100\n\nflows_erasmus_full_zeros |> \n  filter(exchanges > 100) |>\n  arrange(-exchanges) |> \n  ungroup() |>\n  select(sending_country_code, receiving_country_code,\n         exchanges) |>\n  chordDiagram()\n\n\n\n\nChord diagram with only flows over 100\n\n\n\n\nVisulaization 2: Network representation We can also represent the countries as nodes in a network, with the student flows representing the links between them. Use package igraph here.\n\nflows_erasmus_full_zeros |>\n  filter(exchanges > 100) |>\n  graph_from_data_frame(directed = TRUE, \n                      vertices=country_labels) |>\n  plot(vertex.size=5)\n\n\n\n\nNetwork representation\n\n\n\n\nNo isolates\n\nflows_erasmus_full_zeros |>\n  filter(exchanges > 75) |>\n  graph_from_data_frame(directed = TRUE) |>\n  plot(vertex.size=5)\nflows_erasmus_full_zeros |>\n  filter(exchanges > 100) |>\n  graph_from_data_frame(directed = TRUE) |>\n  plot(vertex.size=5)\n\n\n\n\n\n\nNo isolates\n\n\n\n\n\n\n\nNo isolates\n\n\n\n\n\n\nFor alternative methods of visualizing mobility while maintaining the geographic relations, see Andrew Wheeler’s 2015 paper in Cartography and Geographic Information Science, at (behind paywall), here"
  },
  {
    "objectID": "posts/09_TT/09_TT_202211.html#references",
    "href": "posts/09_TT/09_TT_202211.html#references",
    "title": "Student mobility in Europe",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/10_TT/10_TT_202301.html",
    "href": "posts/10_TT/10_TT_202301.html",
    "title": "EDA on salary survey in different countries",
    "section": "",
    "text": "In this tutorial we will do some descriptive analysis on a salary survey in different countries and some background variables of the participants [@yasrebi-de_kom_nsc-r_2023].\n\n\nThank you to Wim Bernasco who provided detailed feedback on the script and supervision on preparing this session."
  },
  {
    "objectID": "posts/10_TT/10_TT_202301.html#getting-started",
    "href": "posts/10_TT/10_TT_202301.html#getting-started",
    "title": "EDA on salary survey in different countries",
    "section": "Getting started",
    "text": "Getting started\nBut let us first make our working environment clean and remove the environment.\n\nrm(list = ls())\n\nWe need two packages here. Install the needed libraries if necessary.\n\n# Install needed libraries\n#install.packages(\"tidyverse\")\n#install.packages(\"priceR\")\n\nThen we have to open the needed packages. We need tidyverse for data wrangling functions (e.g. readr) and we need priceR for handling price and currency data necessary for this dataset and the questions you have to answer.\n\nlibrary(tidyverse)   \nlibrary(priceR)      \n\nNow you can load the salary survey data.\n\nsurvey <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-18/survey.csv')"
  },
  {
    "objectID": "posts/10_TT/10_TT_202301.html#analyses",
    "href": "posts/10_TT/10_TT_202301.html#analyses",
    "title": "EDA on salary survey in different countries",
    "section": "Analyses",
    "text": "Analyses\n\nQuestion 1: What kind of variables are in the dataset?\n\n\n\n\n\n\nNote\n\n\n\n‘|>’ is equal to ‘%>%’, the latter is included in tidyverse\n\n\nLet us look first at the names of the variables.\n\nsurvey |> names() \n\n [1] \"timestamp\"                               \n [2] \"how_old_are_you\"                         \n [3] \"industry\"                                \n [4] \"job_title\"                               \n [5] \"additional_context_on_job_title\"         \n [6] \"annual_salary\"                           \n [7] \"other_monetary_comp\"                     \n [8] \"currency\"                                \n [9] \"currency_other\"                          \n[10] \"additional_context_on_income\"            \n[11] \"country\"                                 \n[12] \"state\"                                   \n[13] \"city\"                                    \n[14] \"overall_years_of_professional_experience\"\n[15] \"years_of_experience_in_field\"            \n[16] \"highest_level_of_education_completed\"    \n[17] \"gender\"                                  \n[18] \"race\"                                    \n\n\nAnd now some information on the variables. What do you see.\n\nsurvey |> dplyr::glimpse()\n\nRows: 26,232\nColumns: 18\n$ timestamp                                <chr> \"4/27/2021 11:02:10\", \"4/27/2~\n$ how_old_are_you                          <chr> \"25-34\", \"25-34\", \"25-34\", \"2~\n$ industry                                 <chr> \"Education (Higher Education)~\n$ job_title                                <chr> \"Research and Instruction Lib~\n$ additional_context_on_job_title          <chr> NA, NA, NA, NA, NA, NA, NA, \"~\n$ annual_salary                            <dbl> 55000, 54600, 34000, 62000, 6~\n$ other_monetary_comp                      <dbl> 0, 4000, NA, 3000, 7000, NA, ~\n$ currency                                 <chr> \"USD\", \"GBP\", \"USD\", \"USD\", \"~\n$ currency_other                           <chr> NA, NA, NA, NA, NA, NA, NA, N~\n$ additional_context_on_income             <chr> NA, NA, NA, NA, NA, NA, NA, N~\n$ country                                  <chr> \"United States\", \"United King~\n$ state                                    <chr> \"Massachusetts\", NA, \"Tenness~\n$ city                                     <chr> \"Boston\", \"Cambridge\", \"Chatt~\n$ overall_years_of_professional_experience <chr> \"5-7 years\", \"8 - 10 years\", ~\n$ years_of_experience_in_field             <chr> \"5-7 years\", \"5-7 years\", \"2 ~\n$ highest_level_of_education_completed     <chr> \"Master's degree\", \"College d~\n$ gender                                   <chr> \"Woman\", \"Non-binary\", \"Woman~\n$ race                                     <chr> \"White\", \"White\", \"White\", \"W~\n\n\nWe have 26,232 rows and 19 columns. All variables, except the variables ‘annual_salary’ and ‘other_monetary_comp’, are text (‘strings’ in SPSS speak).\n\n\nQuestion 2: How many people participated in the survey?**\nYou can count the number of rows.\n\nsurvey |> nrow()\n\n[1] 26232\n\n\nUsing glimpse already indicated that there are 26.232, since one row often means one participant.\n\n\nQuestion 3: what was the distribution of the ages of the people that participated in the survey?\nLet us first count the age groups using variable ‘how_old_are_you’.\n\nsurvey |> count(how_old_are_you)\n\n# A tibble: 7 x 2\n  how_old_are_you     n\n  <chr>           <int>\n1 18-24            1015\n2 25-34           11748\n3 35-44            9398\n4 45-54            3042\n5 55-64             931\n6 65 or over         88\n7 under 18           10\n\n\nBy using sort=TRUE you sort the groups from biggest to smallest group.\n\nsurvey |> count(how_old_are_you, sort = TRUE)  \n\n# A tibble: 7 x 2\n  how_old_are_you     n\n  <chr>           <int>\n1 25-34           11748\n2 35-44            9398\n3 45-54            3042\n4 18-24            1015\n5 55-64             931\n6 65 or over         88\n7 under 18           10\n\n\nNow it is ordered on number. How can you order it on age group, how can we fix this? Order the categories in this variable. For this you have to construct a factor version of ‘how_old_are_you’\n\nsurvey <-\n  survey |>\n  mutate(age_category = factor(how_old_are_you)) \n\nThen,show that they are different types (character vs. factor), but contain the same information\n\nsurvey |>\n  select(how_old_are_you, age_category) |>\n  head() \n\n# A tibble: 6 x 2\n  how_old_are_you age_category\n  <chr>           <fct>       \n1 25-34           25-34       \n2 25-34           25-34       \n3 25-34           25-34       \n4 25-34           25-34       \n5 25-34           25-34       \n6 25-34           25-34       \n\n\nReorder the factor variable now in a generic way (without a function call)\n\nsurvey <-\n  survey |>\n  mutate(age_category = relevel(age_category,\n                                             \"under 18\",\n                                             \"18-24\",\n                                             \"25-34\",\n                                             \"35-44\",\n                                             \"45-54\",\n                                             \"55-64\",\n                                             \"65 or over\"))\n\nThe same result, but easier is the following:\n\nsurvey <-\n  survey |>\n  mutate(age_category = relevel(age_category,\n                                \"under 18\",\n                                \"18-24\"))\n\nNow check if it looks as desired.\n\nsurvey|>\n  count(age_category)\n\n# A tibble: 7 x 2\n  age_category     n\n  <fct>        <int>\n1 under 18        10\n2 18-24         1015\n3 25-34        11748\n4 35-44         9398\n5 45-54         3042\n6 55-64          931\n7 65 or over      88\n\n\nSave this in a variable and l us research the age groups\n#| warning: false\n#| echo: true\nage <- survey |> # save it in a variable\n  count(age_category)\nPut it in a table directly.\n\n\n\nage-category\nnumber\n\n\n\n\n< 18\n10\n\n\n18-24\n1015\n\n\n25-34\n11748\n\n\n35-44\n9398\n\n\n45-54\n3042\n\n\n55-64\n931\n\n\n> 64\n88\n\n\n————–\n——–\n\n\nTotal\n26232\n\n\n\nYou can also write it to Excel if this is easier for you. The info in quotation marks has to be changed to a folder that fits your set-up. Illustrate in Excel: data -> text to columns.\nwrite_csv(age,\"C:/Users/FY125/Desktop/temp/Own tidy tuesday/age.csv\") \nMaybe this one works for you.\nwrite_delim(age,\"C:/Users/FY125/Desktop/temp/Own tidy tuesday/age_delim.csv\", delim = \";\") \nThen you automatically get a nice csv. It depends on windows installation which of the two options work\n\n\nUse knitr::kable(), kableExtra, DataTable::DT(),gt, apaTable or papaja packages for example to write tables for articles directly!\n\n\n\n\n\n\nImportant\n\n\n\nSo far so good - any questions?\n\n\n\n\nPlotting\nPlotting would also be a good idea also for showing distributions.\n\nsurvey |>\n  ggplot() +\n  geom_bar(aes(y = age_category),   # age_category variable on the y axis\n           color=\"black\", fill = \"white\")\n\n\n\n\nFigure 1: Age categories and their numbers\n\n\n\n\nYou also can do it with the blue colorline filled with redcolor.\n\nsurvey |>\n  ggplot() +\n  geom_bar(aes(y = age_category),   # age_category variable on the y axis\n           color=\"blue\", fill = \"red\")\n\n\n\n\nFigure 2: Age categories and there numbers using colors\n\n\n\n\nBut, let us go back to first table and add title and change axis-titles now.\n\n\nsurvey |>\n  ggplot() +\n  geom_bar(aes(y = age_category),   # age_category variable on the y axis\n           color=\"black\", fill = \"white\") +\n  labs(x =\"Number of participants\", y = \"Age category\", fill = \"Age category\")\n\n\n\n\n(a) Participants age\n\n\n\nFigure 3: Age categories and there numbers\n\n\n\n\nQuestion 4: make a dataframe with the variables age, gender, annual salary, currency, other_monetary_comp\nWe call the new dataframe dt and use the function select of the tidyverse package.\n\ndf <- survey |> dplyr::select(age_category, gender, annual_salary, currency, other_monetary_comp)\n\nLook in your environment if you see the new dataframe df with the five variables.\nQuestion 5: What is the distribution of gender in percentages\nUse the df data, count gender, make a new variable percent.\n\ndf |>\n  count(gender) |> \n  mutate(percent = n/sum(n)*100) |> \n  dplyr::select(-n) # not yet rounded\n\n# A tibble: 6 x 2\n  gender                         percent\n  <chr>                            <dbl>\n1 Man                           18.1    \n2 Non-binary                     2.72   \n3 Other or prefer not to answer  1.02   \n4 Prefer not to answer           0.00381\n5 Woman                         77.6    \n6 <NA>                           0.564  \n\n\nn is not-rounded. Show the percentage now rounded (digits=3)\n\ndf |>\n  count(gender) |> \n  mutate(percent = round((n/sum(n)*100), digits = 3)) |> \n  dplyr::select(-n) \n\n# A tibble: 6 x 2\n  gender                        percent\n  <chr>                           <dbl>\n1 Man                            18.1  \n2 Non-binary                      2.72 \n3 Other or prefer not to answer   1.02 \n4 Prefer not to answer            0.004\n5 Woman                          77.6  \n6 <NA>                            0.564\n\n\nOf course, there are always better ways.\n\ndf |>\n  count(gender) |> \n  mutate(percent = (n/sum(n))*100,\n         percent = round(percent,3)) \n\n# A tibble: 6 x 3\n  gender                            n percent\n  <chr>                         <int>   <dbl>\n1 Man                            4743  18.1  \n2 Non-binary                      713   2.72 \n3 Other or prefer not to answer   268   1.02 \n4 Prefer not to answer              1   0.004\n5 Woman                         20359  77.6  \n6 <NA>                            148   0.564\n\n\n\n\nRemember how we wrote this kind of table to Excel: try at home!\n\n\nQuestion 6: Get the min, max, mean, sd, median, IQR and amount of missings for the variable annual salary\nTry first with one or two variables, to make it easy for yourself, for example the meanand the median\n\ndf|> \n  summarise(mean = mean(annual_salary),\n            median = median(annual_salary)) \n\n# A tibble: 1 x 2\n     mean median\n    <dbl>  <dbl>\n1 145725.  76000\n\n\nA different way that can be useful for multiple variables is this one.\n\npv <- df |> dplyr::select(annual_salary) |> \n  pivot_longer(cols = annual_salary, names_to = \"variable_name\", values_to = \"value\")\n\n\ndescriptive_stat <- pv |> \n  group_by(variable_name) |> \n  summarise(min = min(value, na.rm = TRUE),\n            max = max(value, na.rm = TRUE),\n            mean = mean(value, na.rm = TRUE),\n            sd   = sd(value, na.rm = TRUE),\n            median = median(value, na.rm = TRUE),\n            iqr = IQR(value, na.rm = TRUE),\n            miss   = sum(is.na(value)))\ndescriptive_stat\n\n# A tibble: 1 x 8\n  variable_name   min       max    mean       sd median   iqr  miss\n  <chr>         <dbl>     <dbl>   <dbl>    <dbl>  <dbl> <dbl> <int>\n1 annual_salary     0 870000000 145725. 5543526.  76000 56000     0\n\n\nWhy do it like this? Because then you can use it for multiple variables. Now we can:\n\npv_2 <- df |> dplyr::select(annual_salary, other_monetary_comp) |>\n  pivot_longer(cols = c(annual_salary, other_monetary_comp), names_to = \"variable_name\", values_to = \"value\")\n\n\ndescriptive_stat_2 <- pv_2 |> \n  group_by(variable_name) |> \n  summarise(min = min(value, na.rm = TRUE),\n            max = max(value, na.rm = TRUE),\n            mean = mean(value, na.rm = TRUE),\n            sd   = sd(value, na.rm = TRUE),\n            median = median(value, na.rm = TRUE),\n            iqr = IQR(value, na.rm = TRUE),\n            miss   = sum(is.na(value))) \n\ndescriptive_stat_2\n\n# A tibble: 2 x 8\n  variable_name         min       max    mean       sd median   iqr  miss\n  <chr>               <dbl>     <dbl>   <dbl>    <dbl>  <dbl> <dbl> <int>\n1 annual_salary           0 870000000 145725. 5543526.  76000 56000     0\n2 other_monetary_comp     0 120000000  18489.  861667.   2000 10000  6785\n\n\n100 variables or even more variables at once: no problem! Try to write it to a csv!\n\n\nQuestion 7: Plot the distribution of annual salary\nWe work out four ways. The first one is simple, but not very informative way. The second one doesn’t work because it is a tibble not a vector. We show only the code of this one. In number three and four we filter to remove outliers above two different numbers.\n\ndf |> pull(annual_salary) |> hist() # simple, but not yet very informative\n#df |> dplyr::select(annual_salary) |> hist() # select would not work because it is a tibble not a vector\ndf  |> filter(annual_salary < 1000000) |> pull(annual_salary) |> hist() # filter to remove outliers above 1000000\ndf  |> filter(annual_salary < 300000) |> pull(annual_salary) |> hist() \n\n\n\n\n\n\n\n(a) Simple non-informative distribution\n\n\n\n\n\n\n\n(b) Distribution with outliers above 1000000\n\n\n\n\n\n\n\n(c) Distribution with outliers above 300000\n\n\n\n\nFigure 4: Plotting distributions annual salary\n\n\n\nIf you prefer using ggplot you can do it like this.\n# not working for me\n#| label: fig-ggplot\n#| fig-cap: \"Plotting distributions annual salary using ggplot\"\ndf |>\n  filter(annual_salary < 300000) \n  ggplot() +\n  geom_histogram(aes(x = annual_salary), color = \"black\", fill=\"white\", binwidth=5000)\n\n\nQuestion 8: Can we get a salary independent of currency?\nIn this case we take out the participants with only USD and CAD currency and converted their ’annual-salary’to USD.\n\nconverted<- df |> filter(currency == \"USD\" | currency == \"CAD\") |>\n  mutate(\n  USD = convert_currencies(\n    price_start = annual_salary,\n    from = currency,\n    to = \"USD\",\n    date = as.Date(\"2019-12-14\"))\n  )\n\nOr convert all the data.\n\nconverted_all<- df |> filter(!currency %in% c(\"AUD/NZD\", \"Other\")) |>\n  mutate(\n    USD = convert_currencies(\n      price_start = annual_salary,\n      from = currency,\n      to = \"USD\",\n      date = lubridate::today())\n  )\n\n\n\nQuestion 9: Compare salary across genders?\nWe use the converted-all dataset, look for data lower than USD-300000 and show the mean\n\nconverted_all |> group_by(gender) |> filter(USD < 300000) |>\n  summarise(mean = mean(USD)) \n\n# A tibble: 6 x 2\n  gender                           mean\n  <chr>                           <dbl>\n1 Man                           104494.\n2 Non-binary                     67571.\n3 Other or prefer not to answer  85085.\n4 Prefer not to answer           88000 \n5 Woman                          80420.\n6 <NA>                           93652.\n\n\nWe plot this, and now we place it in the margin.\n\nconverted_all |> filter(USD < 300000) |> group_by(gender) |> \n  ggplot(aes(x = as.factor(gender), y = USD)) +\n  geom_boxplot()\n\n\n\n\nFigure 5: Salary across genders"
  },
  {
    "objectID": "posts/10_TT/10_TT_202301.html#references",
    "href": "posts/10_TT/10_TT_202301.html#references",
    "title": "EDA on salary survey in different countries",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/06_TT/06_TT_202204.html",
    "href": "posts/06_TT/06_TT_202204.html",
    "title": "Long to wide (and vv) transformations",
    "section": "",
    "text": "In this session, Sam Langton demonstrated long to wide (and wide to long) transformations using functions available in the tidyr package using data from the London Fire Brigade [@s_langton_nsc-r_2022-1]."
  },
  {
    "objectID": "posts/06_TT/06_TT_202204.html#preparation",
    "href": "posts/06_TT/06_TT_202204.html#preparation",
    "title": "Long to wide (and vv) transformations",
    "section": "Preparation",
    "text": "Preparation\n\nlibrary(tidytuesdayR)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\nIdentify datasets available (fire service data).\n\ntt_df <- tidytuesdayR::tt_load(\"2021-06-29\")\n\n\n    Downloading file 1 of 1: `animal_rescues.csv`\n\n\nPull out the data.\n\nfire_df <- tt_df$animal_rescues\n\nSave for train working.\nHarrie: don't know what this means\nsave.image(\"data/train_working.Rdata\")\nload(\"data/train_working.Rdata\")\nBasic cleaning.\n\nfire_clean_df <- fire_df %>% \n  filter(cal_year < 2021) %>% \n  mutate(animal_group_broad = fct_lump(animal_group_parent, n = 5))"
  },
  {
    "objectID": "posts/06_TT/06_TT_202204.html#transformations",
    "href": "posts/06_TT/06_TT_202204.html#transformations",
    "title": "Long to wide (and vv) transformations",
    "section": "Transformations",
    "text": "Transformations\nCreate some wide data. Freq counts of incidents for each animal by year, then wide. 2021 is not complete. Note animal category is just for the demo!\nTake care, this is a crude recode.\n\nfire_wide1_df <- fire_clean_df %>% \n  filter(cal_year < 2021) %>% \n  select(incident_number, cal_year, animal_group_broad) %>% \n  group_by(cal_year, animal_group_broad) %>% \n  summarise(yearly_count = n()) %>% \n  pivot_wider(names_from = cal_year, values_from = yearly_count)\n\nNon-longitudinal example. Note the fiddly way of keeping zero counts. Requires an ungroup() and then complete(). There might be a better way of doing this!\n\nfire_wide2_df <- fire_clean_df %>% \n  filter(cal_year == 2009) %>% \n  select(incident_number, animal_group_broad, property_category)  %>% \n  group_by(animal_group_broad, property_category) %>% \n  summarise(yearly_count = n()) %>% \n  ungroup() %>% \n  complete(animal_group_broad, property_category, fill = list(yearly_count = 0)) %>% \n  pivot_wider(names_from = property_category, values_from = yearly_count)\n\nNested data / multilevel example.\n\nfire_wards_df <- fire_clean_df %>%\n  group_by(borough_code, ward_code, cal_year) %>% \n  summarise(yearly_count = n()) %>% \n  ungroup() %>% \n  filter(ward_code != \"NULL\") %>% \n  arrange(borough_code, ward_code, cal_year)\n\nMake wide for example, fill in zeros and arrange. Why interesting? Because it’s nested levels, longitudinal and ‘year’ in the var names.\n\nfire_wide3_df <- fire_wards_df %>% \n  arrange(cal_year) %>% \n  pivot_wider(names_from = cal_year, values_from = yearly_count, names_prefix = \"year_\") %>% \n  mutate_if(is.numeric, ~replace_na(., 0)) %>% \n  arrange(ward_code) \n\nRemove things that might confuse people.\n\nrm(fire_clean_df, fire_df, tt_df, fire_wards_df)"
  },
  {
    "objectID": "posts/06_TT/06_TT_202204.html#references",
    "href": "posts/06_TT/06_TT_202204.html#references",
    "title": "Long to wide (and vv) transformations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html",
    "href": "posts/07_TT/07_TT_202205.html",
    "title": "Revenue and expenditure in sport",
    "section": "",
    "text": "The dataset for this Tidy Tuesday is about Collegiate Sports in US. Alex Trinidad explores how revenue and expenditure are distributed in sports. He also looks at the differences in sport revenues and expenditures between men and women.`He presented this on May 3th 2022 in the NSC-R Tidy Tuesday serie. Here you can find the original post [@trinidad_nsc-r_2022]"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html#load-packages-and-importing-data",
    "href": "posts/07_TT/07_TT_202205.html#load-packages-and-importing-data",
    "title": "Revenue and expenditure in sport",
    "section": "Load packages and importing data",
    "text": "Load packages and importing data\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\n\nIdentify TidyTuesday data sets in 2022.\n\ntidytuesdayR::tt_datasets(\"2022\")\n\n   Week       Date                                   Data\n1     1 2022-01-04         Bring your own data from 2022!\n2     2 2022-01-11                      Bee Colony losses\n3     3 2022-01-18                  Chocolate Bar ratings\n4     4 2022-01-25                            Board games\n5     5 2022-02-01                             Dog breeds\n6     6 2022-02-08                        Tuskegee Airmen\n7     7 2022-02-15                   #DuBoisChallenge2022\n8     8 2022-02-22                    World Freedom index\n9     9 2022-03-01              Alternative Fuel Stations\n10   10 2022-03-08               Erasmus student mobility\n11   11 2022-03-15                    CRAN/BIOC Vignettes\n12   12 2022-03-22                             Baby names\n13   13 2022-03-29              Collegiate Sports Budgets\n14   14 2022-04-05                   Digital Publications\n15   15 2022-04-12                   Indoor Air Pollution\n16   16 2022-04-19            Crossword Puzzles and Clues\n17   17 2022-04-26                     Kaggle Hidden Gems\n18   18 2022-05-03                   Solar/Wind utilities\n19   19 2022-05-10                   NYTimes best sellers\n20   20 2022-05-17                             Eurovision\n21   21 2022-05-24                          Women's Rugby\n22   22 2022-05-31                Company reputation poll\n23   23 2022-06-07 Pride Corporate Accountability Project\n24   24 2022-06-14                             US Drought\n25   25 2022-06-21                             Juneteenth\n26   26 2022-06-28                      UK Gender pay gap\n27   27 2022-07-05                  San Francisco Rentals\n28   28 2022-07-12                       European flights\n29   29 2022-07-19                    Technology Adoption\n30   30 2022-07-26                    Bring your own data\n31   31 2022-08-02                    Oregon Spotted Frog\n32   32 2022-08-09                          Ferris Wheels\n33   33 2022-08-16              Open Source Psychometrics\n34   34 2022-08-23                           CHIP dataset\n35   35 2022-08-30                            Pell Grants\n36   36 2022-09-06                          LEGO database\n37   37 2022-09-13                                Bigfoot\n38   38 2022-09-20                Hydro Wastewater plants\n39   39 2022-09-27                     Artists in the USA\n40   40 2022-10-04                  Product Hunt products\n41   41 2022-10-11                           Ravelry data\n42   42 2022-10-18               Stranger things dialogue\n43   43 2022-10-25                  Great British Bakeoff\n44   44 2022-11-01                          Horror Movies\n45   45 2022-11-08                         Radio Stations\n46   46 2022-11-15                       Web page metrics\n47   47 2022-11-22                             UK Museums\n48   48 2022-11-29                         FIFA World Cup\n49   49 2022-12-06                              Elevators\n50   50 2022-12-13             Monthly State Retail Sales\n51   51 2022-12-20              Weather Forecast Accuracy\n52   52 2022-12-27                    Star Trek Timelines\n                                              Source\n1                                                   \n2                                               USDA\n3                                   Flavors of Cacao\n4                                             Kaggle\n5                               American Kennel Club\n6  Commemorative Airforce (CAF) by way of the VA-TUG\n7                                     Anthony Starks\n8                               UN and Freedom House\n9                                             US DOT\n10                                    Data.Europa.eu\n11                              Robert Flight GitHub\n12                       US babynames &  nzbabynames\n13                 Equity in Athletics Data Analysis\n14                                     Project Oasis\n15                                OurWorldInData.org\n16                             Cryptics.georgeho.org\n17                                            Kaggle\n18                                      Berkeley Lab\n19                                       Post45 Data\n20                                        Eurovision\n21                       Women's Rugby - ScrumQueens\n22                             Axios and Harris Poll\n23                                 Data For Progress\n24                                       Drought.gov\n25                WEB DuBois style by Anthony Starks\n26                     gender-pay-gap.service.gov.uk\n27                                   Kate Pennington\n28                                       Eurocontrol\n29                                     data.nber.org\n30                                              None\n31                        usgs.gov spotted frog data\n32                                      ferriswheels\n33                 Open-Source Psychometrics Project\n34                                      CHIP Dataset\n35                              US Dept of Education\n36                                       rebrickable\n37                                        Data.World\n38                                Macedo et al, 2022\n39                                          arts.gov\n40                                    components.one\n41                                       ravelry.com\n42                                         8flix.com\n43                                       bakeoff pkg\n44                                The Movie Database\n45                                         Wikipedia\n46                                   httpArchive.org\n47                  MuseWeb by way of Data Is Plural\n48                             Kaggle FIFA World Cup\n49                                    Elevators data\n50      US Census Bureau Monthly State Retails Sales\n51                Weather Forecast  Capstone Project\n52                                     rtrek package\n                                                         Article\n1                                                               \n2                                                   Bee Informed\n3                                       Will Canniford on Kaggle\n4                                                Alyssa Goldberg\n5                                                            Vox\n6               Wikipedia & Air Force Historical Research Agency\n7                                             Nightingale by DVS\n8                                                  Freedom House\n9                                                            EIA\n10                                                      Wimdu.co\n11                                          Robert Flight GitHub\n12                            Emily Kothe's nzbabynames vignette\n13                                                           NPR\n14                                          Project Oasis Report\n15                                            OurWorldInData.org\n16                                          Towards Data Science\n17                                Kaggle - Notebooks of the Week\n18                                           Berkeley Lab report\n19        Finding Trends in NY Times Best Sellers - Kailey Smith\n20                                                 Tanya Shapiro\n21                                                   ScrumQueens\n22                                               The Harris Poll\n23                                             Data For Progress\n24                                            Drought.gov report\n25                        Isabella Benabaye's blog on Juneteenth\n26                                                    ons.gov.uk\n27                                               Matrix-Berkeley\n28                                                  ec.europa.eu\n29                                                 www.cgdev.org\n30                                                          None\n31                                 usgs.gov spotted-frog-article\n32                                                  ferriswheels\n33                                         Character Personality\n34                                                   arxiv paper\n35                                                pell R package\n36                                                   rebrickable\n37                                               Finding Bigfoot\n38                                               HydroWASTE v1.0\n39                                      Artists in the Workforce\n40                 The Gamer and the Nihilist by Andrew Thompson\n41                                           {ravelRy} R package\n42                               freeCodeCamp & 'stringr things'\n43 Data Visualization in the Tidyverse - The Great Tidy Plot Off\n44                                 Tanya Shapiro's Horror Movies\n45                         Visualizing the Geography of FM Radio\n46                                  DataWrapper & Data is Plural\n47                                          MuseWeb Key Findings\n48                                             Dataset Notebooks\n49                           Elevators data package and examples\n50               Interactive Visualization from US Census Bureau\n51                            Weather Forecast  Capstone Project\n52                                                 rtrek package\n\n\nDownload data set. Note: As list.\n\nttdata <- tidytuesdayR::tt_load(x = 2022, week = 13)\n\n\n    Downloading file 1 of 1: `sports.csv`\n\n\nSelect data set of interest.\n\nsportdt <- ttdata[[1]]\n\nAlternative\nsportdt <- ttdata$sports"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html#data-exploration",
    "href": "posts/07_TT/07_TT_202205.html#data-exploration",
    "title": "Revenue and expenditure in sport",
    "section": "Data Exploration",
    "text": "Data Exploration\nExplore data set\n\nglimpse(sportdt)\n\nRows: 132,327\nColumns: 28\n$ year                 <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2~\n$ unitid               <dbl> 100654, 100654, 100654, 100654, 100654, 100654, 1~\n$ institution_name     <chr> \"Alabama A & M University\", \"Alabama A & M Univer~\n$ city_txt             <chr> \"Normal\", \"Normal\", \"Normal\", \"Normal\", \"Normal\",~\n$ state_cd             <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"~\n$ zip_text             <chr> \"35762\", \"35762\", \"35762\", \"35762\", \"35762\", \"357~\n$ classification_code  <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1~\n$ classification_name  <chr> \"NCAA Division I-FCS\", \"NCAA Division I-FCS\", \"NC~\n$ classification_other <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n$ ef_male_count        <dbl> 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1923, 1~\n$ ef_female_count      <dbl> 2300, 2300, 2300, 2300, 2300, 2300, 2300, 2300, 2~\n$ ef_total_count       <dbl> 4223, 4223, 4223, 4223, 4223, 4223, 4223, 4223, 4~\n$ sector_cd            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ sector_name          <chr> \"Public, 4-year or above\", \"Public, 4-year or abo~\n$ sportscode           <dbl> 1, 2, 3, 7, 8, 15, 16, 22, 26, 33, 1, 2, 3, 8, 12~\n$ partic_men           <dbl> 31, 19, 61, 99, 9, NA, NA, 7, NA, NA, 32, 13, NA,~\n$ partic_women         <dbl> NA, 16, 46, NA, NA, 21, 25, 10, 16, 9, NA, 20, 68~\n$ partic_coed_men      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n$ partic_coed_women    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\n$ sum_partic_men       <dbl> 31, 19, 61, 99, 9, 0, 0, 7, 0, 0, 32, 13, 0, 10, ~\n$ sum_partic_women     <dbl> 0, 16, 46, 0, 0, 21, 25, 10, 16, 9, 0, 20, 68, 7,~\n$ rev_men              <dbl> 345592, 1211095, 183333, 2808949, 78270, NA, NA, ~\n$ rev_women            <dbl> NA, 748833, 315574, NA, NA, 410717, 298164, 13114~\n$ total_rev_menwomen   <dbl> 345592, 1959928, 498907, 2808949, 78270, 410717, ~\n$ exp_men              <dbl> 397818, 817868, 246949, 3059353, 83913, NA, NA, 9~\n$ exp_women            <dbl> NA, 742460, 251184, NA, NA, 432648, 340259, 11388~\n$ total_exp_menwomen   <dbl> 397818, 1560328, 498133, 3059353, 83913, 432648, ~\n$ sports               <chr> \"Baseball\", \"Basketball\", \"All Track Combined\", \"~\n\n\nSelect variables of interest and define chr-variables as fct\n\nttdt_selection <- sportdt %>% \n  dplyr::select(year, institution_name, classification_name, partic_men, partic_women,\n         ef_male_count, ef_female_count, ef_total_count, rev_men,\n         rev_women,total_rev_menwomen, exp_men, exp_women,\n         total_exp_menwomen, sports) %>% \n  mutate(year = as.factor(year),\n         institution_name = as.factor(institution_name),\n         classification_name = as.factor(classification_name),\n         sports = as.factor(sports),\n         total_par = partic_men + partic_women) \n\nNow we can answer some questions:\nHow many years?\n\nsum(table(unique(ttdt_selection$year)))\n\n[1] 5\n\n\nOr:\n\nsum(table(fct_unique(ttdt_selection$year)))\n\n[1] 5\n\n\nHow many divisions?\n\nsum(table(unique(ttdt_selection$classification_name)))\n\n[1] 19\n\n\nHow may institutions?\n\nsum(table(unique(ttdt_selection$institution_name)))\n\n[1] 2212\n\n\nHow many sports?\n\nsum(table(unique(ttdt_selection$sports)))\n\n[1] 38\n\n\nHow many cases per wave?\n\nttdt_selection %>% \n  count(year)\n\n# A tibble: 5 x 2\n  year      n\n  <fct> <int>\n1 2015  17345\n2 2016  17414\n3 2017  17628\n4 2018  17772\n5 2019  62168\n\n\nHow many cases per sport?\n\nttdt_selection %>% \n  count(sports)\n\n# A tibble: 38 x 2\n   sports                 n\n   <fct>              <int>\n 1 All Track Combined  4870\n 2 Archery             1557\n 3 Badminton           1554\n 4 Baseball            8644\n 5 Basketball         10000\n 6 Beach Volleyball    1988\n 7 Bowling             2176\n 8 Diving              1530\n 9 Equestrian          1799\n10 Fencing             1687\n# ... with 28 more rows"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html#visualizations",
    "href": "posts/07_TT/07_TT_202205.html#visualizations",
    "title": "Revenue and expenditure in sport",
    "section": "Visualizations",
    "text": "Visualizations\nPlot measures per sport\n\nggplot(data = ttdt_selection) +\n  geom_bar(mapping = aes(x = sports, color = sports))  +\n  theme(legend.position = \"none\")\n\n\n\n\nProportions in a bar chart\n\n\n\n\nPlot measures per sport (y axis)\n\nggplot(data = ttdt_selection) + \n  geom_bar(mapping = aes(y = sports, color = sports))\n\n\n\n\nMeasures per sport\n\n\n\n\nPlot measures per sport (y axis ordered infrequent).\n\nggplot(data = ttdt_selection) + \n  geom_bar(mapping = aes(y = fct_infreq(sports), color = sports))\n\n\n\n\nMeasures per sport ordered infrequent\n\n\n\n\nPlot measures per sport (y)\n\nggplot(data = ttdt_selection) + \n  geom_bar(mapping = aes(y = fct_rev(fct_infreq(sports)), color = sports))\n\n\n\n\nMeasures per sport ordered 2\n\n\n\n\nPlot measures per sport (y)\n\nggplot(data = ttdt_selection) + \n  geom_bar(mapping = aes(y = fct_rev(fct_infreq(sports)), color = sports)) +\n  ylab(\"Sports\") \n\n\n\n\nMeasures per sport ordered 3\n\n\n\n\nPlot measures per sport (per year)\n\nggplot(data = ttdt_selection) + \n  geom_bar(mapping = aes(y = fct_rev(fct_infreq(sports)), color = sports)) +\n  ylab(\"Sports\") +\n  facet_wrap(vars(year)) +\n  theme(legend.position = \"none\")\n\n\n\n\nMeasures per sport via facet wrap"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html#missing-data",
    "href": "posts/07_TT/07_TT_202205.html#missing-data",
    "title": "Revenue and expenditure in sport",
    "section": "Missing data",
    "text": "Missing data\nIs any NA in any of my variables?\n\nsummary(ttdt_selection)\n\n   year                  institution_name \n 2015:17345   Westminster College:   238  \n 2016:17414   Union College      :   233  \n 2017:17628   Columbia College   :   187  \n 2018:17772   Bethel University  :   181  \n 2019:62168   Marian University  :   166  \n              Emmanuel College   :   162  \n              (Other)            :131160  \n                         classification_name   partic_men      partic_women   \n NCAA Division III with football   :18835    Min.   :  1.00   Min.   :  1.00  \n NCAA Division III without football:12310    1st Qu.: 13.00   1st Qu.: 11.00  \n NJCAA Division I                  :11831    Median : 22.00   Median : 16.00  \n NCAA Division II with football    :11535    Mean   : 30.86   Mean   : 20.71  \n NCAA Division I-FBS               :10052    3rd Qu.: 35.00   3rd Qu.: 23.00  \n NCAA Division II without football : 9571    Max.   :331.00   Max.   :327.00  \n (Other)                           :58193    NA's   :70462    NA's   :63442   \n ef_male_count   ef_female_count ef_total_count     rev_men         \n Min.   :    0   Min.   :    0   Min.   :    0   Min.   :       65  \n 1st Qu.:  513   1st Qu.:  652   1st Qu.: 1194   1st Qu.:    63428  \n Median :  986   Median : 1248   Median : 2259   Median :   158126  \n Mean   : 2126   Mean   : 2496   Mean   : 4622   Mean   :   809011  \n 3rd Qu.: 2385   3rd Qu.: 2860   3rd Qu.: 5237   3rd Qu.:   400604  \n Max.   :35954   Max.   :30325   Max.   :66279   Max.   :156147208  \n                                                 NA's   :70462      \n   rev_women        total_rev_menwomen     exp_men           exp_women      \n Min.   :       0   Min.   :      130   Min.   :      65   Min.   :     65  \n 1st Qu.:   58746   1st Qu.:    96299   1st Qu.:   63062   1st Qu.:  59301  \n Median :  138318   Median :   228776   Median :  159666   Median : 141800  \n Mean   :  279346   Mean   :   795231   Mean   :  662386   Mean   : 331594  \n 3rd Qu.:  331120   3rd Qu.:   541876   3rd Qu.:  424025   3rd Qu.: 361860  \n Max.   :21440365   Max.   :156147208   Max.   :69718059   Max.   :9485162  \n NA's   :63444      NA's   :45193       NA's   :70462      NA's   :63442    \n total_exp_menwomen        sports        total_par     \n Min.   :     130   Basketball:10000   Min.   :  2.00  \n 1st Qu.:   96436   Volleyball: 9122   1st Qu.: 22.00  \n Median :  234559   Soccer    : 8647   Median : 32.00  \n Mean   :  732422   Baseball  : 8644   Mean   : 45.66  \n 3rd Qu.:  585604   Softball  : 8560   3rd Qu.: 53.00  \n Max.   :69718059   Golf      : 7060   Max.   :617.00  \n NA's   :45191      (Other)   :80294   NA's   :88713   \n\n\nRemove NAs from revenues in men and women.\n\nmyselection <- ttdt_selection %>% \n  filter(!rev_men %in% NA & !rev_women %in% NA)\n\nCheck if NA’s in myselection dataset.\n\nsummary(myselection)\n\n   year                 institution_name\n 2015:8559   Westminster College:  103  \n 2016:8628   Bethel University  :   84  \n 2017:8767   Union College      :   84  \n 2018:8880   Emmanuel College   :   79  \n 2019:8780   Harvard University :   75  \n             Marian University  :   73  \n             (Other)            :43116  \n                         classification_name   partic_men      partic_women   \n NCAA Division III with football   : 8268    Min.   :  1.00   Min.   :  1.00  \n NCAA Division III without football: 5186    1st Qu.: 11.00   1st Qu.: 10.00  \n NCAA Division II without football : 3575    Median : 17.00   Median : 15.00  \n NCAA Division II with football    : 3415    Mean   : 24.18   Mean   : 21.48  \n NAIA Division II                  : 3313    3rd Qu.: 28.00   3rd Qu.: 25.00  \n NCAA Division I-FCS               : 3048    Max.   :290.00   Max.   :327.00  \n (Other)                           :16809                                     \n ef_male_count   ef_female_count ef_total_count     rev_men        \n Min.   :    0   Min.   :    0   Min.   :    0   Min.   :      65  \n 1st Qu.:  546   1st Qu.:  684   1st Qu.: 1268   1st Qu.:   55012  \n Median : 1004   Median : 1272   Median : 2284   Median :  131951  \n Mean   : 2140   Mean   : 2493   Mean   : 4633   Mean   :  405014  \n 3rd Qu.: 2393   3rd Qu.: 2830   3rd Qu.: 5237   3rd Qu.:  309113  \n Max.   :35954   Max.   :30325   Max.   :66279   Max.   :45632816  \n                                                                   \n   rev_women        total_rev_menwomen    exp_men           exp_women      \n Min.   :       0   Min.   :     130   Min.   :      65   Min.   :     65  \n 1st Qu.:   51180   1st Qu.:  108178   1st Qu.:   54786   1st Qu.:  51228  \n Median :  122982   Median :  259386   Median :  134146   Median : 125092  \n Mean   :  269807   Mean   :  674821   Mean   :  392666   Mean   : 319436  \n 3rd Qu.:  299104   3rd Qu.:  618145   3rd Qu.:  331960   3rd Qu.: 323727  \n Max.   :21440365   Max.   :48559421   Max.   :22178473   Max.   :9485162  \n                                                                           \n total_exp_menwomen                        sports        total_par     \n Min.   :     130   Basketball                : 9448   Min.   :  2.00  \n 1st Qu.:  107800   Soccer                    : 6657   1st Qu.: 22.00  \n Median :  261562   Tennis                    : 4628   Median : 32.00  \n Mean   :  712101   Golf                      : 4258   Mean   : 45.66  \n 3rd Qu.:  659871   All Track Combined        : 3604   3rd Qu.: 53.00  \n Max.   :28847845   Track and Field, X-Country: 3442   Max.   :617.00  \n                    (Other)                   :11577                   \n\n\nAlternative way\n\ntable(is.na(myselection))\n\n\n FALSE \n697824"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html#revenues-and-expenditures",
    "href": "posts/07_TT/07_TT_202205.html#revenues-and-expenditures",
    "title": "Revenue and expenditure in sport",
    "section": "Revenues and expenditures",
    "text": "Revenues and expenditures\nCalculate revenues and expenditure per participant and add new variables.\n\nmyselection <- myselection %>% \n  mutate(exp_per_men = exp_men / partic_men,\n         exp_per_women = exp_women / partic_women,\n         exp_per_total = total_exp_menwomen / total_par, \n         rev_per_men = rev_men / partic_men,\n         rev_per_women = rev_women / partic_women,\n         rev_per_total = total_rev_menwomen / total_par)\n\nRevenues\nNow look at revenue in sports (Mean revenues per sport). This will not work.\n\nrev_mean <- myselection %>% \n  group_by(sports) %>% \n  summarise(mean_rev_total = mean(total_rev_menwomen)) %>% \n      ggplot(aes(x = mean_rev_total, y = sports, color = sports)) +\n      geom_bar() +\n      labs(x = \"Mean Revenues\", y = \"Sports\") \n\nGet rid of scientific notation\n\noptions(scipen = 999)\n\nOr activate scientific notation\n\noptions(scipen = 0)\n\nSolution change to stat = “identity” in geom_bar()\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_rev_total = mean(total_rev_menwomen)) %>% \n  ggplot(aes(x = mean_rev_total, y = sports, color = sports)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Mean Revenues\", y = \"Sports\") \n\n\n\n\nOrdering bars now\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_rev_total = mean(total_rev_menwomen)) %>% \n  ggplot(aes(x = mean_rev_total, y = fct_rev(fct_infreq(sports)), color = sports)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Mean Revenues\", y = \"Sports\") \n\n\n\n\nRevenues per sport ordered\n\n\n\n\nBars reordered.\n\nmyselection %>% \n  group_by(year, sports) %>% \n  summarise(mean_rev_total = mean(total_rev_menwomen)) %>% \n  ggplot(aes(x = mean_rev_total, y = reorder(sports, mean_rev_total),\n             color = sports)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Mean Revenues\", y = \"Sports\") + \n  theme(legend.position = \"none\") + \n  facet_wrap(vars(year))\n\n\n\n\nRevenues per sport bars reordered\n\n\n\n\nPlot mean revenues per sport and sex.\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_rev_men = mean(rev_men),\n            mean_rev_women = mean(rev_women)) %>% \n  pivot_longer(cols = c(mean_rev_men,mean_rev_women), names_to = \"sex\",\n               values_to = \"mean_rev\") %>% \n  ggplot(aes(x = mean_rev, y = reorder(sports, mean_rev), fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Mean Revenues\", y = \"Sports\", fill = \"Sex\") +\n  scale_fill_discrete(labels = c(\"Men\", \"Women\"))\n\n\n\n\nRevenues per sport and sex\n\n\n\n\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_rev_men = mean(rev_men),\n            mean_rev_women = mean(rev_women)) %>% \n  mutate(mean_dif = sqrt((mean_rev_men - mean_rev_women) ^ 2)) %>% \n  ggplot(aes(x = mean_dif, y = reorder(sports, mean_dif), fill = mean_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences in Revenues (USD)\",  y = \"Sports\", fill = \"USD\")  \n\n\n\n\nMean sex differences in revenues (USD)\n\n\n\n\nExpenditures in Sport\nPlot mean expenditure\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_men = mean(exp_men),\n            mean_exp_women = mean(exp_women)) %>% \n  pivot_longer(cols = c(mean_exp_men,mean_exp_women), names_to = \"sex\",\n               values_to = \"mean_exp\") %>% \n  ggplot(aes(x = mean_exp, y = reorder(sports, mean_exp), fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Mean Expenditure\", y = \"Sports\", fill = \"Sex\") +\n  scale_fill_discrete(labels = c(\"Men\", \"Women\"))\n\n\n\n\nExpenditures in sport\n\n\n\n\nPlotting mean differences by sex.\n\nmyselection %>% \n  group_by(sports) %>% # if facet_wrap, add year\n  summarise(mean_exp_men = mean(exp_men),\n            mean_exp_women = mean(exp_women)) %>% \n  mutate(mean_dif = sqrt((mean_exp_men - mean_exp_women) ^ 2)) %>% \n  ggplot(aes(x = mean_dif, y = reorder(sports, mean_dif), fill = mean_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences (USD)\",  y = \"Sports\", fill = \"USD\")  \n\n\n\n\nMean differences by sex\n\n\n\n\nIf necessary install RColorBrewer package\n# install.packages(RColorBrewer) \n\nlibrary(RColorBrewer)\n\nSet palettes (display.brewer.all())\n\ndiscrete_palettes <- list(\n  c(\"orange\", \"skyblue\"),\n  RColorBrewer::brewer.pal(6, \"Accent\"),\n  RColorBrewer::brewer.pal(3, \"Set2\")\n)\n\nCalculate mean expenditure per participant & plot.\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_pamen = mean(exp_per_men),\n            mean_exp_pawomen = mean(exp_per_women)) %>%  \n  pivot_longer(cols = c(mean_exp_pamen,mean_exp_pawomen), names_to = \"sex\",\n               values_to = \"mean_exp_pa\") %>% \n  ggplot(aes(x = mean_exp_pa, y = reorder(sports, mean_exp_pa), fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Year and Institution Mean Expenditure per Participant\",\n       y = \"Sports\", fill = \"Sex\") +\n  scale_fill_discrete(labels = c(\"Men\", \"Women\"), type = discrete_palettes)\n\n\n\n\nYear and institution mean expenditure per participant\n\n\n\n\nCalculate mean expenditure per participant differences and plot\n\nmyselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_pamen = mean(exp_per_men),\n            mean_exp_pawomen = mean(exp_per_women)) %>% \n  mutate(mean_pa_dif = sqrt((mean_exp_pamen - mean_exp_pawomen) ^ 2)) %>% \n  ggplot(aes(x = mean_pa_dif, y = reorder(sports, mean_pa_dif), \n             fill = mean_pa_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences Expenditures per Participant (USD)\", \n       y = \"Sports\", fill = \"USD\") +\n  scale_fill_continuous( type = \"viridis\")\n\n\n\n\nMean Sex Differences Expenditures per Participant (USD)\n\n\n\n\nCompare plots with means: Expenditure “Gross” & per participant\n\nplotmeanexp <- myselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_men = mean(exp_men),\n            mean_exp_women = mean(exp_women)) %>% \n  pivot_longer(cols = c(mean_exp_men,mean_exp_women), names_to = \"sex\",\n               values_to = \"mean_exp\") %>% \n  ggplot(aes(x = mean_exp, y = reorder(sports, mean_exp), fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Year and Institution Mean Expenditure\", y = \"Sports\", fill = \"Sex\") +\n  scale_fill_discrete(labels = c(\"Men\", \"Women\"))\nplotmeanexp\n\n\n\n\nYear and Institution Mean Expenditure\n\n\n\n\n\nplotmeanexp_pa <- myselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_pamen = mean(exp_per_men),\n            mean_exp_pawomen = mean(exp_per_women)) %>%  \n  pivot_longer(cols = c(mean_exp_pamen,mean_exp_pawomen), names_to = \"sex\",\n               values_to = \"mean_exp_pa\") %>% \n  ggplot(aes(x = mean_exp_pa, y = reorder(sports, mean_exp_pa), fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Year and Institution Mean Expenditure per Participant\",\n       y = \"Sports\", fill = \"Sex\") +\n  scale_fill_discrete(labels = c(\"Men\", \"Women\"), type = discrete_palettes)\nplotmeanexp_pa\n\n\n\n\nYear and Institution Mean Expenditure per Participant\n\n\n\n\n\nplotmeandifexp <- myselection %>% \n  group_by(sports) %>% # if facet_wrap, add year\n  summarise(mean_exp_men = mean(exp_men),\n            mean_exp_women = mean(exp_women)) %>% \n  mutate(mean_dif = sqrt((mean_exp_men - mean_exp_women) ^ 2)) %>% \n  ggplot(aes(x = mean_dif, y = reorder(sports, mean_dif), fill = mean_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences in Expenditures (USD)\",  y = \"Sports\", fill = \"USD\")\nplotmeandifexp\n\n\n\n\nMean Sex Differences in Expenditures\n\n\n\n\n\nplotmeandifexp_pa <- myselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_pamen = mean(exp_per_men),\n            mean_exp_pawomen = mean(exp_per_women)) %>% \n  mutate(mean_pa_dif = sqrt((mean_exp_pamen - mean_exp_pawomen) ^ 2)) %>% \n  ggplot(aes(x = mean_pa_dif, y = reorder(sports, mean_pa_dif), \n             fill = mean_pa_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences Expenditures per Participant (USD)\", \n       y = \"Sports\", fill = \"USD\") +\n  scale_fill_continuous( type = \"viridis\")\n\nIf necessary install package\ninstall.packages(\"gridExtra\")\nLoad package\n\nlibrary(gridExtra)\n\nPlots together to compare\n\ngridExtra::grid.arrange(plotmeanexp, plotmeanexp_pa)\n\n\n\n\nComparing plots\n\n\n\ngridExtra::grid.arrange(plotmeandifexp, plotmeandifexp_pa)\n\n\n\n\nComparing plots\n\n\n\n\nRelationship between expenditure and revenue\n\nplotmeandifexp_pa <- myselection %>% \n  group_by(sports) %>% \n  summarise(mean_exp_pamen = mean(exp_per_men),\n            mean_exp_pawomen = mean(exp_per_women)) %>% \n  mutate(mean_pa_dif = sqrt((mean_exp_pamen - mean_exp_pawomen) ^ 2)) %>% \n  ggplot(aes(x = mean_pa_dif, y = reorder(sports, mean_pa_dif), \n             fill = mean_pa_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences Expenditures per Participant (USD)\", \n       y = \"Sports\", fill = \"USD\") +\n  scale_fill_continuous( type = \"viridis\")\n\n\nplotmeandifrev_pa <- myselection %>% \n  group_by(sports) %>% \n  summarise(mean_rev_pamen = mean(rev_per_men),\n            mean_rev_pawomen = mean(rev_per_women)) %>% \n  mutate(mean_parev_dif = sqrt((mean_rev_pamen - mean_rev_pawomen) ^ 2)) %>% \n  ggplot(aes(x = mean_parev_dif, y = reorder(sports, mean_parev_dif), \n             fill = mean_parev_dif)) +\n  geom_bar(stat = \"identity\") +\n  # facet_wrap(vars(year)) +\n  labs(x = \"Mean Sex Differences Revenues per Participant (USD)\", \n       y = \"Sports\", fill = \"USD\") \n\nGrid plot\n\ngridExtra::grid.arrange(plotmeandifrev_pa, plotmeandifexp_pa)\n\n\n\n\nGrid plots compare\n\n\n\n\nCorrelation between Expenditures and Revenues\n\ncor(myselection$exp_men, myselection$rev_men, method = \"spearman\")\n\n[1] 0.9642041\n\n\nCorrelation between exp. and rev. per sport.\n\nmyselection %>% \n  group_by(sports) %>%\n  summarise(assoc_exp_rev_men = cor(exp_men, rev_men, method = \"spearman\"))\n\n# A tibble: 31 x 2\n   sports             assoc_exp_rev_men\n   <fct>                          <dbl>\n 1 All Track Combined            0.855 \n 2 Archery                       0.991 \n 3 Basketball                    0.996 \n 4 Beach Volleyball              0.987 \n 5 Bowling                       0.996 \n 6 Diving                        0.531 \n 7 Equestrian                    0.5   \n 8 Fencing                       0.696 \n 9 Golf                          0.914 \n10 Gymnastics                    0.0757\n# ... with 21 more rows\n\n\nPlot association\n\nmyselection %>% \n  group_by(sports) %>%\n  ggplot(mapping = aes(x = exp_men, y = rev_men)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Men Expenditure\", \n       y = \"Men Revenue\", fill = \"USD\") +\n  facet_wrap(vars(sports), scales = \"free_y\")\n\n\n\n\nPlot association\n\n\n\n\n\nmyselection %>% \n  group_by(sports) %>%\n  ggplot(mapping = aes(x = exp_women, y = rev_women)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(x = \"Women Expenditure\", \n       y = \"Women Revenue\", fill = \"USD\") +\n  facet_wrap(vars(sports), scales = \"free_y\")\n\n\n\n\nRelationship woman expenditure and revenue"
  },
  {
    "objectID": "posts/07_TT/07_TT_202205.html#references",
    "href": "posts/07_TT/07_TT_202205.html#references",
    "title": "Revenue and expenditure in sport",
    "section": "References",
    "text": "References"
  }
]